{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "sidebar_position: 1\n",
    "keywords: [conversationchain]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建一个聊天机器人"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::note\n",
    "\n",
    "本教程之前使用了 [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) 抽象。您可以在 [v0.2 文档](https://python.langchain.com/v0.2/docs/tutorials/chatbot/) 中访问该版本的文档。\n",
    "\n",
    "从 LangChain 的 v0.3 版本开始，我们建议用户利用 [LangGraph 持久化](https://langchain-ai.github.io/langgraph/concepts/persistence/) 将 `memory` 集成到新的 LangChain 应用中。\n",
    "\n",
    "如果您的代码已经依赖于 `RunnableWithMessageHistory` 或 `BaseChatMessageHistory`，您无需进行任何更改。我们近期没有计划弃用此功能，因为它适用于简单的聊天应用，任何使用 `RunnableWithMessageHistory` 的代码将继续正常工作。\n",
    "\n",
    "请参阅 [如何迁移到 LangGraph Memory](/docs/versions/migrating_memory/) 了解更多详细信息。\n",
    ":::\n",
    "\n",
    "## 概述\n",
    "\n",
    "我们将通过一个示例来学习如何设计和实现一个基于 LLM 的聊天机器人。\n",
    "这个聊天机器人将能够与 [聊天模型](/docs/concepts/chat_models) 进行对话并记住之前的交互。\n",
    "\n",
    "请注意，我们构建的这个聊天机器人仅使用语言模型进行对话。\n",
    "您可能正在寻找以下几个相关概念：\n",
    "\n",
    "- [对话式 RAG](/docs/tutorials/qa_chat_history)：在外部数据源上启用聊天机器人体验\n",
    "- [代理](/docs/tutorials/agents)：构建可以执行操作的聊天机器人\n",
    "\n",
    "本教程将涵盖基础知识，这些知识对于上述两个更高级的主题非常有用，但如果您愿意，可以直接跳到那里。\n",
    "\n",
    "## 设置\n",
    "\n",
    "### Jupyter Notebook\n",
    "\n",
    "本指南（以及文档中的大多数其他指南）使用 [Jupyter notebooks](https://jupyter.org/) 并假设读者也在使用。Jupyter notebooks 非常适合学习如何使用 LLM 系统，因为有时可能会出现问题（意外输出、API 停止工作等），在交互式环境中学习指南是更好地理解它们的好方法。\n",
    "\n",
    "本教程和其他教程可能最方便在 Jupyter notebook 中运行。请参阅 [此处](https://jupyter.org/install) 了解安装说明。\n",
    "\n",
    "### 安装\n",
    "\n",
    "对于本教程，我们需要 `langchain-core` 和 `langgraph`。本指南需要 `langgraph >= 0.2.28`。\n",
    "\n",
    "import Tabs from '@theme/Tabs';\n",
    "import TabItem from '@theme/TabItem';\n",
    "import CodeBlock from \"@theme/CodeBlock\";\n",
    "\n",
    "<Tabs>\n",
    "  <TabItem value=\"pip\" label=\"Pip\" default>\n",
    "    <CodeBlock language=\"bash\">pip install langchain-core langgraph>0.2.27</CodeBlock>\n",
    "  </TabItem>\n",
    "  <TabItem value=\"conda\" label=\"Conda\">\n",
    "    <CodeBlock language=\"bash\">conda install langchain-core langgraph>0.2.27 -c conda-forge</CodeBlock>\n",
    "  </TabItem>\n",
    "</Tabs>\n",
    "\n",
    "\n",
    "\n",
    "有关更多详细信息，请参阅我们的 [安装指南](/docs/how_to/installation)。\n",
    "\n",
    "### LangSmith\n",
    "\n",
    "您使用 LangChain 构建的许多应用程序将包含多个步骤和多次 LLM 调用。\n",
    "随着这些应用程序变得越来越复杂，能够检查链或代理内部发生的确切情况变得至关重要。\n",
    "最好的方法是使用 [LangSmith](https://smith.langchain.com)。\n",
    "\n",
    "在上面的链接注册后，请确保设置环境变量以开始记录跟踪：\n",
    "\n",
    "```shell\n",
    "export LANGSMITH_TRACING=\"true\"\n",
    "export LANGSMITH_API_KEY=\"...\"\n",
    "```\n",
    "\n",
    "或者，如果在 notebook 中，您可以使用以下方式设置它们：\n",
    "\n",
    "```python\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n",
    "```\n",
    "\n",
    "## 快速入门\n",
    "\n",
    "首先，让我们学习如何单独使用语言模型。LangChain 支持许多不同的语言模型，您可以互换使用它们 - 选择您想要使用的模型！\n",
    "\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs overrideParams={{openai: {model: \"gpt-4o-mini\"}}} />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先直接使用模型。`ChatModel` 是 LangChain \"Runnables\" 的实例，这意味着它们提供了一个标准接口来与之交互。要简单地调用模型，我们可以将消息列表传递给 `.invoke` 方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi Bob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-5211544f-da9f-4325-8b8e-b3d92b2fc71a-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"你好！我是 Bob\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型本身没有任何状态的概念。例如，如果您问一个后续问题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry, but I don't have access to personal information about users unless it has been shared with me in the course of our conversation. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 11, 'total_tokens': 45, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-a2d13a18-7022-4784-b54f-f85c097d1075-0', usage_metadata={'input_tokens': 11, 'output_tokens': 34, 'total_tokens': 45, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"我的名字是什么？\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们看看示例 [LangSmith 跟踪](https://smith.langchain.com/public/5c21cb92-2814-4119-bae9-d02b8db577ac/r)\n",
    "\n",
    "我们可以看到它没有将之前的对话轮次纳入上下文，无法回答问题。\n",
    "这会导致糟糕的聊天机器人体验！\n",
    "\n",
    "为了解决这个问题，我们需要将整个 [对话历史](/docs/concepts/chat_history) 传递给模型。让我们看看这样做会发生什么："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Bob! How can I help you today, Bob?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 33, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-34bcccb3-446e-42f2-b1de-52c09936c02c-0', usage_metadata={'input_tokens': 33, 'output_tokens': 14, 'total_tokens': 47, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"你好！我是 Bob\"),\n",
    "        AIMessage(content=\"你好 Bob！今天我能为您做些什么？\"),\n",
    "        HumanMessage(content=\"我的名字是什么？\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以看到我们得到了一个好的响应！\n",
    "\n",
    "这是聊天机器人能够进行对话的基本理念。\n",
    "那么我们如何最好地实现这一点呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 消息持久化\n",
    "\n",
    "[LangGraph](https://langchain-ai.github.io/langgraph/) 实现了一个内置的持久化层，非常适合支持多轮对话的聊天应用。\n",
    "\n",
    "将我们的聊天模型包装在一个最小的 LangGraph 应用中，可以自动持久化消息历史，简化多轮应用的开发。\n",
    "\n",
    "LangGraph 配备了一个简单的内存检查点，我们在下面使用它。请参阅其 [文档](https://langchain-ai.github.io/langgraph/concepts/persistence/) 了解更多详细信息，包括如何使用不同的持久化后端（例如 SQLite 或 Postgres）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# 定义一个新的图\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# 定义调用模型的函数\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# 定义图中的（单个）节点\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# 添加内存\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在需要创建一个 `config`，每次传递给 runnable。此配置包含不是直接输入的一部分但仍然有用的信息。在本例中，我们希望包括一个 `thread_id`。这应该看起来像："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这使我们能够通过单个应用支持多个对话线程，这是当您的应用有多个用户时的常见需求。\n",
    "\n",
    "然后我们可以调用应用程序："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Bob! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"你好！我是 Bob。\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # 输出包含状态中的所有消息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob! How can I help you today, Bob?\n"
     ]
    }
   ],
   "source": [
    "query = \"我的名字是什么？\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "太棒了！我们的聊天机器人现在记得关于我们的信息。如果我们更改配置以引用不同的 `thread_id`，我们可以看到它重新开始对话。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I don't have access to personal information about you unless you've shared it in this conversation. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是，我们可以随时回到原始对话（因为我们将其持久化在数据库中）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob. What would you like to discuss today?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这就是我们如何支持聊天机器人与许多用户进行对话的方式！\n",
    "\n",
    ":::tip\n",
    "\n",
    "对于异步支持，将 `call_model` 节点更新为异步函数，并在调用应用程序时使用 `.ainvoke`：\n",
    "\n",
    "```python\n",
    "# 节点的异步函数：\n",
    "async def call_model(state: MessagesState):\n",
    "    response = await model.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# 像之前一样定义图：\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# 异步调用：\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们所做的只是围绕模型添加了一个简单的持久化层。我们可以通过添加提示模板使聊天机器人更加复杂和个性化。\n",
    "\n",
    "## 提示模板\n",
    "\n",
    "[提示模板](/docs/concepts/prompt_templates) 有助于将原始用户信息转换为 LLM 可以处理的格式。在本例中，原始用户输入只是一个消息，我们将其传递给 LLM。现在让我们使其稍微复杂一些。首先，我们添加一个带有一些自定义指令的系统消息（但仍然将消息作为输入）。接下来，我们将添加除消息之外的更多输入。\n",
    "\n",
    "要添加系统消息，我们将创建一个 `ChatPromptTemplate`。我们将利用 `MessagesPlaceholder` 来传递所有消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"你像海盗一样说话。尽最大努力回答所有问题。\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在可以更新我们的应用程序以合并此模板："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    # highlight-start\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    # highlight-end\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们以相同的方式调用应用程序："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ahoy there, Jim! What brings ye to these waters today? Be ye seekin' treasure, knowledge, or perhaps a good tale from the high seas? Arrr!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"你好！我是 Jim。\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ye be called Jim, matey! A fine name fer a swashbuckler such as yerself! What else can I do fer ye? Arrr!\n"
     ]
    }
   ],
   "source": [
    "query = \"我的名字是什么？\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "太棒了！现在让我们的提示稍微复杂一些。假设提示模板现在看起来像这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"你是一个乐于助人的助手。用 {language} 尽最大努力回答所有问题。\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，我们已向提示添加了一个新的 `language` 输入。我们的应用程序现在有两个参数——输入 `messages` 和 `language`。我们应该更新应用程序的状态以反映这一点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "# highlight-next-line\n",
    "class State(TypedDict):\n",
    "    # highlight-next-line\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    # highlight-next-line\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "¡Hola, Bob! ¿Cómo puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"你好！我是 Bob。\"\n",
    "language = \"西班牙语\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    # highlight-next-line\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，整个状态是持久化的，因此如果不需要更改，可以省略像 `language` 这样的参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Tu nombre es Bob. ¿Hay algo más en lo que pueda ayudarte?\n"
     ]
    }
   ],
   "source": [
    "query = \"我的名字是什么？\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了帮助您理解内部发生的事情，请查看 [此 LangSmith 跟踪](https://smith.langchain.com/public/15bd8589-005c-4812-b9b9-23e74ba4c3c6/r)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 管理对话历史\n",
    "\n",
    "构建聊天机器人时需要理解的一个重要概念是如何管理对话历史。如果不加以管理，消息列表将无限增长并可能溢出 LLM 的上下文窗口。因此，重要的是添加一个步骤来限制您传递给模型的消息大小。\n",
    "\n",
    "**重要的是，您需要在提示模板之前但在从消息历史加载之前的消息之后执行此操作。**\n",
    "\n",
    "我们可以通过在提示前添加一个简单的步骤来修改 `messages` 键，然后将该新链包装在消息历史类中。\n",
    "\n",
    "LangChain 附带了一些用于 [管理消息列表](/docs/how_to/#messages) 的内置助手。在本例中，我们将使用 [trim_messages](/docs/how_to/trim_messages/) 助手来减少我们发送给模型的消息数量。修剪器允许我们指定要保留的令牌数量，以及其他参数，例如是否始终保留系统消息以及是否允许部分消息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"你是一个好助手\"),\n",
    "    HumanMessage(content=\"你好！我是 Bob\"),\n",
    "    AIMessage(content=\"你好！\"),\n",
    "    HumanMessage(content=\"我喜欢香草冰淇淋\"),\n",
    "    AIMessage(content=\"不错\"),\n",
    "    HumanMessage(content=\"2 + 2 是多少\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"谢谢\"),\n",
    "    AIMessage(content=\"没问题！\"),\n",
    "    HumanMessage(content=\"玩得开心吗？\"),\n",
    "    AIMessage(content=\"是的！\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要在我们的链中使用它，我们只需要在将 `messages` 输入传递给提示之前运行修剪器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    # highlight-start\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    # highlight-end\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，如果我们尝试询问模型我们的名字，它不会知道，因为我们修剪了聊天历史的那部分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't know your name. You haven't told me yet!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"我的名字是什么？\"\n",
    "language = \"英语\"\n",
    "\n",
    "# highlight-next-line\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是，如果我们询问最近几条消息中的信息，它会记住："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You asked what 2 + 2 equals.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "query = \"我问了什么数学问题？\"\n",
    "language = \"英语\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您查看 LangSmith，您可以在 [LangSmith 跟踪](https://smith.langchain.com/public/04402eaa-29e6-4bb1-aa91-885b730b6c21/r) 中准确了解内部发生的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 流式传输\n",
    "\n",
    "现在我们有了一个功能齐全的聊天机器人。然而，对于聊天机器人应用来说，一个非常重要的用户体验考虑是流式传输。LLM 有时可能需要一段时间才能响应，因此为了改善用户体验，大多数应用程序所做的一件事是流式传输每个生成的令牌。这使用户能够看到进度。\n",
    "\n",
    "实际上，这非常容易实现！\n",
    "\n",
    "默认情况下，我们的 LangGraph 应用中的 `.stream` 会流式传输应用步骤——在本例中，模型响应的单个步骤。设置 `stream_mode=\"messages\"` 允许我们流式传输输出令牌："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Hi| Todd|!| Here|’s| a| joke| for| you|:\n",
      "\n",
      "|Why| don|’t| skeleton|s| fight| each| other|?\n",
      "\n",
      "|Because| they| don|’t| have| the| guts|!||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"你好，我是 Todd，请告诉我一个笑话。\"\n",
    "language = \"英语\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "# highlight-next-line\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # 过滤到仅模型响应\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下一步\n",
    "\n",
    "现在您了解了如何在 LangChain 中创建聊天机器人的基础知识，您可能感兴趣的一些更高级的教程是：\n",
    "\n",
    "- [对话式 RAG](/docs/tutorials/qa_chat_history)：在外部数据源上启用聊天机器人体验\n",
    "- [代理](/docs/tutorials/agents)：构建可以执行操作的聊天机器人\n",
    "\n",
    "如果您想深入了解具体内容，值得查看的一些内容是：\n",
    "\n",
    "- [流式传输](/docs/how_to/streaming)：流式传输对聊天应用至关重要\n",
    "- [如何添加消息历史](/docs/how_to/message_history)：深入了解与消息历史相关的所有内容\n",
    "- [如何管理大型消息历史](/docs/how_to/trim_messages/)：管理大型聊天历史的更多技术\n",
    "- [LangGraph 主文档](https://langchain-ai.github.io/langgraph/)：了解有关使用 LangGraph 构建的更多详细信息"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
