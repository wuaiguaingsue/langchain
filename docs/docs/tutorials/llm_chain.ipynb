{
 "cells": [
  {
   "cell_type": "raw",
   "id": "63ee3f93",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 0\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9316da0d",
   "metadata": {},
   "source": [
    "# 构建一个简单的LLM应用程序，使用聊天模型和提示模板\n",
    "\n",
    "在这个快速入门中，我们将向您展示如何使用LangChain构建一个简单的LLM应用程序。这个应用程序将把文本从英语翻译成另一种语言。这是一个相对简单的LLM应用程序——它只是一个单一的LLM调用加上一些提示。然而，这是开始使用LangChain的一个好方法——许多功能可以通过一些提示和一个LLM调用来构建！\n",
    "\n",
    "阅读本教程后，您将对以下内容有一个高层次的了解：\n",
    "\n",
    "- 使用[语言模型](/docs/concepts/chat_models)\n",
    "\n",
    "- 使用[提示模板](/docs/concepts/prompt_templates)\n",
    "\n",
    "- 使用[LangSmith](https://docs.smith.langchain.com/)调试和跟踪您的应用程序\n",
    "\n",
    "让我们开始吧！\n",
    "\n",
    "## 设置\n",
    "\n",
    "### Jupyter Notebook\n",
    "\n",
    "本教程和其他教程可能最方便在[Jupyter notebooks](https://jupyter.org/)中运行。在交互式环境中浏览指南是更好地理解它们的好方法。请参阅[此处](https://jupyter.org/install)了解安装说明。\n",
    "\n",
    "### 安装\n",
    "\n",
    "要安装LangChain，请运行：\n",
    "\n",
    "<!-- HIDE_IN_NB\n",
    "import Tabs from '@theme/Tabs';\n",
    "import TabItem from '@theme/TabItem';\n",
    "import CodeBlock from \"@theme/CodeBlock\";\n",
    "\n",
    "<Tabs>\n",
    "  <TabItem value=\"pip\" label=\"Pip\" default>\n",
    "    <CodeBlock language=\"bash\">pip install langchain</CodeBlock>\n",
    "  </TabItem>\n",
    "  <TabItem value=\"conda\" label=\"Conda\">\n",
    "    <CodeBlock language=\"bash\">conda install langchain -c conda-forge</CodeBlock>\n",
    "  </TabItem>\n",
    "</Tabs>\n",
    "HIDE_IN_NB -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86874822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "\n",
    "# %pip install langchain\n",
    "# 或\n",
    "# %conda install langchain -c conda-forge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a546a5bc",
   "metadata": {},
   "source": [
    "有关更多详细信息，请参阅我们的[安装指南](/docs/how_to/installation)。\n",
    "\n",
    "### LangSmith\n",
    "\n",
    "您使用LangChain构建的许多应用程序将包含多个步骤和多个LLM调用的调用。\n",
    "随着这些应用程序变得越来越复杂，能够检查链或代理内部究竟发生了什么变得至关重要。\n",
    "最好的方法是使用[LangSmith](https://smith.langchain.com)。\n",
    "\n",
    "在上面的链接注册后，请确保设置您的环境变量以开始记录跟踪：\n",
    "\n",
    "```shell\n",
    "export LANGSMITH_TRACING=\"true\"\n",
    "export LANGSMITH_API_KEY=\"...\"\n",
    "export LANGSMITH_PROJECT=\"default\" # 或任何其他项目名称\n",
    "```\n",
    "\n",
    "或者，如果在笔记本中，您可以使用以下方式设置它们：\n",
    "\n",
    "```python\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # 从.env文件加载环境变量（需要`python-dotenv`）\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\n",
    "        prompt=\"输入您的LangSmith API密钥（可选）：\"\n",
    "    )\n",
    "if \"LANGSMITH_PROJECT\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass(\n",
    "        prompt='输入您的LangSmith项目名称（默认=\"default\"）：'\n",
    "    )\n",
    "    if not os.environ.get(\"LANGSMITH_PROJECT\"):\n",
    "        os.environ[\"LANGSMITH_PROJECT\"] = \"default\"\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\n",
    "        prompt=\"输入您的OpenAI API密钥（如果使用OpenAI则必需）：\"\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5558ca9",
   "metadata": {},
   "source": [
    "## 使用语言模型\n",
    "\n",
    "首先，让我们学习如何单独使用语言模型。LangChain支持许多不同的语言模型，您可以互换使用它们。有关使用特定模型的详细信息，请参阅[支持的集成](/docs/integrations/chat/)。\n",
    "\n",
    "<!-- HIDE_IN_NB>\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs overrideParams={{openai: {model: \"gpt-4o-mini\"}}} />\n",
    "HIDE_IN_NB -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b41234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5642ff",
   "metadata": {},
   "source": [
    "让我们首先直接使用模型。[ChatModels](/docs/concepts/chat_models)是LangChain [Runnables](/docs/concepts/runnables/)的实例，这意味着它们提供了一个标准接口来与它们交互。为了简单地调用模型，我们可以将一组[消息](/docs/concepts/messages/)传递给`.invoke`方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2481f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-32654a56-627c-40e1-a141-ad9350bbfd3e-0', usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"将以下内容从英语翻译成意大利语\"),\n",
    "    HumanMessage(\"你好！\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83373db",
   "metadata": {},
   "source": [
    ":::提示\n",
    "\n",
    "如果我们启用了LangSmith，我们可以看到此运行已记录到LangSmith，并可以查看[LangSmith跟踪](https://smith.langchain.com/public/88baa0b2-7c1a-4d09-ba30-a47985dde2ea/r)。LangSmith跟踪报告了[令牌](/docs/concepts/tokens/)使用信息、延迟、[标准模型参数](/docs/concepts/chat_models/#standard-parameters)（例如温度）以及其他信息。\n",
    "\n",
    ":::\n",
    "\n",
    "请注意，ChatModels接收[消息](/docs/concepts/messages/)对象作为输入，并生成消息对象作为输出。除了文本内容外，消息对象还传递对话[角色](/docs/concepts/messages/#role)并保存重要数据，例如[工具调用](/docs/concepts/tool_calling/)和令牌使用计数。\n",
    "\n",
    "LangChain还支持通过字符串或[OpenAI格式](/docs/concepts/messages/#openai-format)的聊天模型输入。以下是等效的：\n",
    "\n",
    "```python\n",
    "model.invoke(\"你好\")\n",
    "\n",
    "model.invoke([{\"role\": \"user\", \"content\": \"你好\"}])\n",
    "\n",
    "model.invoke([HumanMessage(\"你好\")])\n",
    "```\n",
    "\n",
    "### 流式传输\n",
    "\n",
    "由于聊天模型是[Runnables](/docs/concepts/runnables/)，它们提供了一个标准接口，包括异步和流式传输调用模式。这使我们能够从聊天模型流式传输单个令牌："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0abb0863-bee7-448d-b013-79d8db01e330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|C|iao|!||"
     ]
    }
   ],
   "source": [
    "for token in model.stream(messages):\n",
    "    print(token.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5963141-468c-4570-8f2e-5f7cfb6eb3db",
   "metadata": {},
   "source": [
    "您可以在[本指南](/docs/how_to/chat_streaming/)中找到有关流式传输聊天模型输出的更多详细信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab8da31",
   "metadata": {},
   "source": [
    "## 提示模板\n",
    "\n",
    "现在我们直接将一组消息传递给语言模型。这组消息来自哪里？通常，它是由用户输入和应用程序逻辑的组合构成的。此应用程序逻辑通常会获取原始用户输入并将其转换为准备传递给语言模型的消息列表。常见的转换包括添加系统消息或使用用户输入格式化模板。\n",
    "\n",
    "[提示模板](/docs/concepts/prompt_templates/)是LangChain中的一个概念，旨在协助完成此转换。它们接收原始用户输入并返回准备传递给语言模型的数据（提示）。\n",
    "\n",
    "让我们在这里创建一个提示模板。它将接收两个用户变量：\n",
    "\n",
    "- `language`：要将文本翻译成的语言\n",
    "- `text`：要翻译的文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e73cc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"将以下内容从英语翻译成{language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e876c2a",
   "metadata": {},
   "source": [
    "请注意，`ChatPromptTemplate`支持单个模板中的多个[消息角色](/docs/concepts/messages/#role)。我们将`language`参数格式化到系统消息中，并将用户`text`格式化到用户消息中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9711ba6",
   "metadata": {},
   "source": [
    "此提示模板的输入是一个字典。我们可以单独使用此提示模板来查看它的作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f781b3cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"意大利语\", \"text\": \"你好！\"})\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a49ba9e",
   "metadata": {},
   "source": [
    "我们可以看到它返回了一个`ChatPromptValue`，其中包含两个消息。如果我们想直接访问消息，我们可以这样做："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2159b619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e70ee6-f0e0-4ae0-a290-002799ebf828",
   "metadata": {},
   "source": [
    "最后，我们可以在格式化的提示上调用聊天模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a509d8c-e122-4641-b9ee-91bc23aa155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao!\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f0bf25-6efb-4853-9a8f-242f2855c84a",
   "metadata": {},
   "source": [
    ":::提示\n",
    "消息`content`可以同时包含文本和具有附加结构的[内容块](/docs/concepts/messages/#aimessage)。有关更多信息，请参阅[本指南](/docs/how_to/output_parser_string/)。\n",
    ":::\n",
    "\n",
    "如果我们查看[LangSmith跟踪](https://smith.langchain.com/public/3ccc2d5e-2869-467b-95d6-33a577df99a2/r)，我们可以准确地看到聊天模型接收到的提示，以及[令牌](/docs/concepts/tokens/)使用信息、延迟、[标准模型参数](/docs/concepts/chat_models/#standard-parameters)（例如温度）以及其他信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befdb168",
   "metadata": {},
   "source": [
    "## 结论\n",
    "\n",
    "就是这样！在本教程中，您已经学习了如何创建您的第一个简单LLM应用程序。您已经学习了如何使用语言模型、如何创建提示模板以及如何通过LangSmith获得对您创建的应用程序的良好可观察性。\n",
    "\n",
    "这只是您想要学习以成为熟练的AI工程师的冰山一角。幸运的是——我们还有很多其他资源！\n",
    "\n",
    "有关LangChain核心概念的进一步阅读，我们有详细的[概念指南](/docs/concepts)。\n",
    "\n",
    "如果您对这些概念有更具体的问题，请查看以下部分的操作指南：\n",
    "\n",
    "- [聊天模型](/docs/how_to/#chat-models)\n",
    "- [提示模板](/docs/how_to/#prompt-templates)\n",
    "\n",
    "以及LangSmith文档：\n",
    "\n",
    "- [LangSmith](https://docs.smith.langchain.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
