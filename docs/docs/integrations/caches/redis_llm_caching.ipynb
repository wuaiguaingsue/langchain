{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain 的 Redis 缓存\n",
    "\n",
    "本笔记本演示如何使用 langchain-redis 包中的 `RedisCache` 和 `RedisSemanticCache` 类来为 LLM 响应实现缓存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置\n",
    "\n",
    "首先，安装所需的依赖项并确保 Redis 实例正在运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain-core langchain-redis langchain-openai redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确保 Redis 服务器正在运行。你可以使用 Docker 启动一个：\n",
    "\n",
    "```\n",
    "docker run -d -p 6379:6379 redis:latest\n",
    "```\n",
    "\n",
    "或者根据你操作系统的说明在本地安装并运行 Redis。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Redis at: redis://redis:6379\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 如果设置了环境变量则使用该变量，否则默认为 localhost\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "print(f\"连接到 Redis：{REDIS_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入所需库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.schema import Generation\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain_redis import RedisCache, RedisSemanticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_core\n",
    "import langchain_openai\n",
    "import openai\n",
    "import redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置 OpenAI API 密钥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key not found in environment variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your OpenAI API key:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key has been set for this session.\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "# 检查环境中是否已设置 OPENAI_API_KEY\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"在环境变量中未找到 OpenAI API 密钥。\")\n",
    "    openai_api_key = getpass(\"请输入您的 OpenAI API 密钥：\")\n",
    "\n",
    "    # 为当前会话设置 API 密钥\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "    print(\"已为本次会话设置 OpenAI API 密钥。\")\n",
    "else:\n",
    "    print(\"在环境变量中找到 OpenAI API 密钥。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 RedisCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call (not cached):\n",
      "Result: \n",
      "\n",
      "Caching is the process of storing frequently accessed data in a temporary storage location for faster retrieval. This helps to reduce the time and resources needed to access the data from its original source. Caching is commonly used in computer systems, web browsers, and databases to improve performance and efficiency.\n",
      "Time: 1.16 seconds\n",
      "\n",
      "Second call (cached):\n",
      "Result: \n",
      "\n",
      "Caching is the process of storing frequently accessed data in a temporary storage location for faster retrieval. This helps to reduce the time and resources needed to access the data from its original source. Caching is commonly used in computer systems, web browsers, and databases to improve performance and efficiency.\n",
      "Time: 0.05 seconds\n",
      "\n",
      "Speed improvement: 25.40x faster\n",
      "Cache cleared\n"
     ]
    }
   ],
   "source": [
    "# 初始化 RedisCache\n",
    "redis_cache = RedisCache(redis_url=REDIS_URL)\n",
    "\n",
    "# 设置 LangChain 使用的缓存\n",
    "set_llm_cache(redis_cache)\n",
    "\n",
    "# 初始化语言模型\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "\n",
    "# 测量执行时间的函数\n",
    "def timed_completion(prompt):\n",
    "    start_time = time.time()\n",
    "    result = llm.invoke(prompt)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time\n",
    "\n",
    "\n",
    "# 第一次调用（未缓存）\n",
    "prompt = \"用三句话解释缓存的概念。\"\n",
    "result1, time1 = timed_completion(prompt)\n",
    "print(f\"第一次调用（未缓存）：\\n结果：{result1}\\n时间：{time1:.2f} 秒\\n\")\n",
    "\n",
    "# 第二次调用（应该被缓存）\n",
    "result2, time2 = timed_completion(prompt)\n",
    "print(f\"第二次调用（已缓存）：\\n结果：{result2}\\n时间：{time2:.2f} 秒\\n\")\n",
    "\n",
    "print(f\"速度提升：快了 {time1 / time2:.2f} 倍\")\n",
    "\n",
    "# 清除缓存\n",
    "redis_cache.clear()\n",
    "print(\"缓存已清除\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 RedisSemanticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query:\n",
      "Prompt: What is the capital of France?\n",
      "Result: \n",
      "\n",
      "The capital of France is Paris.\n",
      "Time: 1.52 seconds\n",
      "\n",
      "Similar query:\n",
      "Prompt: Can you tell me the capital city of France?\n",
      "Result: \n",
      "\n",
      "The capital of France is Paris.\n",
      "Time: 0.29 seconds\n",
      "\n",
      "Speed improvement: 5.22x faster\n",
      "Semantic cache cleared\n"
     ]
    }
   ],
   "source": [
    "# 初始化 RedisSemanticCache\n",
    "embeddings = OpenAIEmbeddings()\n",
    "semantic_cache = RedisSemanticCache(\n",
    "    redis_url=REDIS_URL, embeddings=embeddings, distance_threshold=0.2\n",
    ")\n",
    "\n",
    "# 设置 LangChain 使用的缓存\n",
    "set_llm_cache(semantic_cache)\n",
    "\n",
    "\n",
    "# 测试语义缓存的函数\n",
    "def test_semantic_cache(prompt):\n",
    "    start_time = time.time()\n",
    "    result = llm.invoke(prompt)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time\n",
    "\n",
    "\n",
    "# 原始查询\n",
    "original_prompt = \"法国的首都是什么？\"\n",
    "result1, time1 = test_semantic_cache(original_prompt)\n",
    "print(\n",
    "    f\"原始查询：\\n提示：{original_prompt}\\n结果：{result1}\\n时间：{time1:.2f} 秒\\n\"\n",
    ")\n",
    "\n",
    "# 语义相似的查询\n",
    "similar_prompt = \"你能告诉我法国的首都城市是什么吗？\"\n",
    "result2, time2 = test_semantic_cache(similar_prompt)\n",
    "print(\n",
    "    f\"相似查询：\\n提示：{similar_prompt}\\n结果：{result2}\\n时间：{time2:.2f} 秒\\n\"\n",
    ")\n",
    "\n",
    "print(f\"速度提升：快了 {time1 / time2:.2f} 倍\")\n",
    "\n",
    "# 清除语义缓存\n",
    "semantic_cache.clear()\n",
    "print(\"语义缓存已清除\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高级用法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义 TTL（生存时间）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached result: Cached response\n",
      "Waiting for TTL to expire...\n",
      "Result after TTL: Not found (expired)\n"
     ]
    }
   ],
   "source": [
    "# 使用自定义 TTL 初始化 RedisCache\n",
    "ttl_cache = RedisCache(redis_url=REDIS_URL, ttl=5)  # 60 秒 TTL\n",
    "\n",
    "# 更新缓存条目\n",
    "ttl_cache.update(\"test_prompt\", \"test_llm\", [Generation(text=\"缓存的响应\")])\n",
    "\n",
    "# 检索缓存的条目\n",
    "cached_result = ttl_cache.lookup(\"test_prompt\", \"test_llm\")\n",
    "print(f\"缓存的结果：{cached_result[0].text if cached_result else '未找到'}\")\n",
    "\n",
    "# 等待 TTL 过期\n",
    "print(\"等待 TTL 过期...\")\n",
    "time.sleep(6)\n",
    "\n",
    "# 尝试检索过期的条目\n",
    "expired_result = ttl_cache.lookup(\"test_prompt\", \"test_llm\")\n",
    "print(\n",
    "    f\"TTL 后的结果：{expired_result[0].text if expired_result else '未找到（已过期）'}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义 RedisSemanticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original result: \n",
      "\n",
      "The largest planet in our solar system is Jupiter.\n",
      "Similar query result: \n",
      "\n",
      "The largest planet in our solar system is Jupiter.\n"
     ]
    }
   ],
   "source": [
    "# 使用自定义设置初始化 RedisSemanticCache\n",
    "custom_semantic_cache = RedisSemanticCache(\n",
    "    redis_url=REDIS_URL,\n",
    "    embeddings=embeddings,\n",
    "    distance_threshold=0.1,  # 更严格的相似度阈值\n",
    "    ttl=3600,  # 1 小时 TTL\n",
    "    name=\"custom_cache\",  # 自定义缓存名称\n",
    ")\n",
    "\n",
    "# 测试自定义语义缓存\n",
    "set_llm_cache(custom_semantic_cache)\n",
    "\n",
    "test_prompt = \"太阳系中最大的行星是什么？\"\n",
    "result, _ = test_semantic_cache(test_prompt)\n",
    "print(f\"原始结果：{result}\")\n",
    "\n",
    "# 尝试略有不同的查询\n",
    "similar_test_prompt = \"太阳系中哪个行星最大？\"\n",
    "similar_result, _ = test_semantic_cache(similar_test_prompt)\n",
    "print(f\"相似查询结果：{similar_result}\")\n",
    "\n",
    "# 清理\n",
    "custom_semantic_cache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结论\n",
    "\n",
    "本笔记本演示了 langchain-redis 包中 `RedisCache` 和 `RedisSemanticCache` 的用法。这些缓存机制可以通过减少冗余的 API 调用并利用语义相似性进行智能缓存，显著提高基于 LLM 的应用程序的性能。基于 Redis 的实现为分布式系统中的缓存提供了快速、可扩展和灵活的解决方案。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
