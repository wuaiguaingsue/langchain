{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confident\n",
    "\n",
    ">[DeepEval](https://confident-ai.com) 是用于LLM单元测试的包。\n",
    "> 使用Confident，每个人都可以通过更快的迭代构建强大的语言模型，\n",
    "> 同时利用单元测试和集成测试。我们为迭代过程中的每个步骤提供支持，\n",
    "> 从合成数据创建到测试。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本指南中，我们将演示如何测试和衡量LLM的性能。我们展示如何使用我们的回调来测量性能，以及如何定义自己的指标并将其记录到我们的仪表板中。\n",
    "\n",
    "DeepEval还提供：\n",
    "- 如何生成合成数据\n",
    "- 如何测量性能\n",
    "- 一个用于监控和随时间查看结果的仪表板"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 安装和设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-openai langchain-community deepeval langchain-chroma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取API凭证\n",
    "\n",
    "要获取DeepEval API凭证，请按照以下步骤操作：\n",
    "\n",
    "1. 访问 https://app.confident-ai.com\n",
    "2. 点击\"Organization\"\n",
    "3. 复制API密钥。\n",
    "\n",
    "\n",
    "登录时，您还将被要求设置`implementation`名称。实现名称是必需的，用于描述实现类型。（考虑您想为项目起什么名字。我们建议使其具有描述性。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!deepeval login"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置DeepEval\n",
    "\n",
    "默认情况下，您可以使用`DeepEvalCallbackHandler`来设置要跟踪的指标。但是，目前对指标的支持有限（很快会添加更多）。目前支持：\n",
    "- [回答相关性](https://docs.confident-ai.com/docs/measuring_llm_performance/answer_relevancy)\n",
    "- [偏见](https://docs.confident-ai.com/docs/measuring_llm_performance/debias)\n",
    "- [毒性](https://docs.confident-ai.com/docs/measuring_llm_performance/non_toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics.answer_relevancy import AnswerRelevancy\n",
    "\n",
    "# 在这里，我们希望确保答案至少是相关的\n",
    "answer_relevancy_metric = AnswerRelevancy(minimum_score=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入门"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要使用`DeepEvalCallbackHandler`，我们需要`implementation_name`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.callbacks.confident_callback import DeepEvalCallbackHandler\n",
    "\n",
    "deepeval_callback = DeepEvalCallbackHandler(\n",
    "    implementation_name=\"langchainQuickstart\", metrics=[answer_relevancy_metric]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 场景1：输入到LLM\n",
    "\n",
    "然后您可以将其输入到您的OpenAI LLM中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\nQ: What did the fish say when he hit the wall? \\nA: Dam.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nThe Moon \\n\\nThe moon is high in the midnight sky,\\nSparkling like a star above.\\nThe night so peaceful, so serene,\\nFilling up the air with love.\\n\\nEver changing and renewing,\\nA never-ending light of grace.\\nThe moon remains a constant view,\\nA reminder of life’s gentle pace.\\n\\nThrough time and space it guides us on,\\nA never-fading beacon of hope.\\nThe moon shines down on us all,\\nAs it continues to rise and elope.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nQ. What did one magnet say to the other magnet?\\nA. \"I find you very attractive!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nThe world is charged with the grandeur of God.\\nIt will flame out, like shining from shook foil;\\nIt gathers to a greatness, like the ooze of oil\\nCrushed. Why do men then now not reck his rod?\\n\\nGenerations have trod, have trod, have trod;\\nAnd all is seared with trade; bleared, smeared with toil;\\nAnd wears man's smudge and shares man's smell: the soil\\nIs bare now, nor can foot feel, being shod.\\n\\nAnd for all this, nature is never spent;\\nThere lives the dearest freshness deep down things;\\nAnd though the last lights off the black West went\\nOh, morning, at the brown brink eastward, springs —\\n\\nBecause the Holy Ghost over the bent\\nWorld broods with warm breast and with ah! bright wings.\\n\\n~Gerard Manley Hopkins\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nQ: What did one ocean say to the other ocean?\\nA: Nothing, they just waved.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\n\\nA poem for you\\n\\nOn a field of green\\n\\nThe sky so blue\\n\\nA gentle breeze, the sun above\\n\\nA beautiful world, for us to love\\n\\nLife is a journey, full of surprise\\n\\nFull of joy and full of surprise\\n\\nBe brave and take small steps\\n\\nThe future will be revealed with depth\\n\\nIn the morning, when dawn arrives\\n\\nA fresh start, no reason to hide\\n\\nSomewhere down the road, there's a heart that beats\\n\\nBelieve in yourself, you'll always succeed.\", generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 504, 'total_tokens': 528, 'prompt_tokens': 24}, 'model_name': 'text-davinci-003'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    temperature=0,\n",
    "    callbacks=[deepeval_callback],\n",
    "    verbose=True,\n",
    "    openai_api_key=\"<YOUR_API_KEY>\",\n",
    ")\n",
    "output = llm.generate(\n",
    "    [\n",
    "        \"什么是最好的评估工具？（完全没有偏见）\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，您可以通过调用`is_successful()`方法检查指标是否成功。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_relevancy_metric.is_successful()\n",
    "# 返回True/False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行完成后，您应该能够在下方看到我们的仪表板。\n",
    "\n",
    "![仪表板](https://docs.confident-ai.com/assets/images/dashboard-screenshot-b02db73008213a211b1158ff052d969e.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 场景2：在没有回调的链中跟踪LLM\n",
    "\n",
    "要在没有回调的链中跟踪LLM，您可以在最后插入它。\n",
    "\n",
    "我们可以先定义一个简单的链，如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_file_url = \"https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt\"\n",
    "\n",
    "openai_api_key = \"sk-XXX\"\n",
    "\n",
    "with open(\"state_of_the_union.txt\", \"w\") as f:\n",
    "    response = requests.get(text_file_url)\n",
    "    f.write(response.text)\n",
    "\n",
    "loader = TextLoader(\"state_of_the_union.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(openai_api_key=openai_api_key),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(),\n",
    ")\n",
    "\n",
    "# 提供一个新的问答管道\n",
    "query = \"谁是总统？\"\n",
    "result = qa.run(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义链后，您可以手动检查答案相似性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_relevancy_metric.measure(result, query)\n",
    "answer_relevancy_metric.is_successful()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下一步？\n",
    "\n",
    "您可以在[这里](https://docs.confident-ai.com/docs/quickstart/custom-metrics)创建自己的自定义指标。\n",
    "\n",
    "DeepEval还提供其他功能，如[自动创建单元测试](https://docs.confident-ai.com/docs/quickstart/synthetic-data-creation)、[幻觉测试](https://docs.confident-ai.com/docs/measuring_llm_performance/factual_consistency)。\n",
    "\n",
    "如果您有兴趣，请查看我们的Github仓库[https://github.com/confident-ai/deepeval](https://github.com/confident-ai/deepeval)。我们欢迎任何PR和关于如何改进LLM性能的讨论。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a53ebf4a859167383b364e7e7521d0add3c2dbbdecce4edf676e8c4634ff3fbb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
