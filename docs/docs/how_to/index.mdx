---
sidebar_position: 0
sidebar_class_name: hidden
---

# 如何操作指南

在这里，您将找到“如何做……？”类型问题的答案。
这些指南是*目标导向*和*具体*的；它们旨在帮助您完成特定任务。
有关概念性解释，请参阅[概念指南](/docs/concepts/)。
有关端到端的演练，请参阅[教程](/docs/tutorials/)。
有关每个类和函数的全面描述，请参阅[API 参考](https://python.langchain.com/api_reference/)。

## 安装

- [如何：安装 LangChain 包](/docs/how_to/installation/)
- [如何：在不同的 Pydantic 版本中使用 LangChain](/docs/how_to/pydantic_compatibility)

## 关键功能

以下是使用 LangChain 的核心功能亮点：

- [如何：从模型返回结构化数据](/docs/how_to/structured_output/)
- [如何：使用模型调用工具](/docs/how_to/tool_calling)
- [如何：流式运行](/docs/how_to/streaming)
- [如何：调试您的 LLM 应用程序](/docs/how_to/debugging/)

## 组件

这些是构建应用程序时可以使用的核心构建块。

### 聊天模型

[聊天模型](/docs/concepts/chat_models) 是一种较新的语言模型形式，它接收消息并输出消息。
有关从特定提供商开始使用聊天模型的详细信息，请参阅[支持的集成](/docs/integrations/chat/)。

- [如何：进行函数/工具调用](/docs/how_to/tool_calling)
- [如何：让模型返回结构化输出](/docs/how_to/structured_output)
- [如何：缓存模型响应](/docs/how_to/chat_model_caching)
- [如何：获取日志概率](/docs/how_to/logprobs)
- [如何：创建自定义聊天模型类](/docs/how_to/custom_chat_model)
- [如何：流式返回响应](/docs/how_to/chat_streaming)
- [如何：跟踪令牌使用情况](/docs/how_to/chat_token_usage_tracking)
- [如何：跨提供商跟踪响应元数据](/docs/how_to/response_metadata)
- [如何：使用聊天模型调用工具](/docs/how_to/tool_calling)
- [如何：流式工具调用](/docs/how_to/tool_streaming)
- [如何：处理速率限制](/docs/how_to/chat_model_rate_limiting)
- [如何：少样本提示工具行为](/docs/how_to/tools_few_shot)
- [如何：绑定特定于模型格式的工具](/docs/how_to/tools_model_specific)
- [如何：强制特定工具调用](/docs/how_to/tool_choice)
- [如何：使用本地模型](/docs/how_to/local_llms)
- [如何：一行代码初始化任何模型](/docs/how_to/chat_models_universal_init/)
- [如何：将多模态数据直接传递给模型](/docs/how_to/multimodal_inputs/)

### 消息

[消息](/docs/concepts/messages) 是聊天模型的输入和输出。它们具有一些 `content` 和一个 `role`，描述消息的来源。

- [如何：修剪消息](/docs/how_to/trim_messages/)
- [如何：过滤消息](/docs/how_to/filter_messages/)
- [如何：合并同类型的连续消息](/docs/how_to/merge_message_runs/)

### 提示模板

[提示模板](/docs/concepts/prompt_templates) 负责将用户输入格式化为可以传递给语言模型的格式。

- [如何：使用少样本示例](/docs/how_to/few_shot_examples)
- [如何：在聊天模型中使用少样本示例](/docs/how_to/few_shot_examples_chat/)
- [如何：部分格式化提示模板](/docs/how_to/prompts_partial)
- [如何：组合提示](/docs/how_to/prompts_composition)
- [如何：使用多模态提示](/docs/how_to/multimodal_prompts/)

### 示例选择器

[示例选择器](/docs/concepts/example_selectors) 负责选择正确的少样本示例以传递给提示。

- [如何：使用示例选择器](/docs/how_to/example_selectors)
- [如何：按长度选择示例](/docs/how_to/example_selectors_length_based)
- [如何：按语义相似性选择示例](/docs/how_to/example_selectors_similarity)
- [如何：按语义 ngram 重叠选择示例](/docs/how_to/example_selectors_ngram)
- [如何：按最大边际相关性选择示例](/docs/how_to/example_selectors_mmr)
- [如何：从 LangSmith 少样本数据集中选择示例](/docs/how_to/example_selectors_langsmith/)

### LLMs

LangChain 所称的 [LLMs](/docs/concepts/text_llms) 是较早形式的语言模型，它们接收字符串输入并输出字符串。

- [如何：缓存模型响应](/docs/how_to/llm_caching)
- [如何：创建自定义 LLM 类](/docs/how_to/custom_llm)
- [如何：流式返回响应](/docs/how_to/streaming_llm)
- [如何：跟踪令牌使用情况](/docs/how_to/llm_token_usage_tracking)
- [如何：使用本地模型](/docs/how_to/local_llms)

### 输出解析器

[输出解析器](/docs/concepts/output_parsers) 负责将 LLM 的输出解析为更结构化的格式。

- [如何：从消息对象中解析文本](/docs/how_to/output_parser_string)
- [如何：使用输出解析器将 LLM 响应解析为结构化格式](/docs/how_to/output_parser_structured)
- [如何：解析 JSON 输出](/docs/how_to/output_parser_json)
- [如何：解析 XML 输出](/docs/how_to/output_parser_xml)
- [如何：解析 YAML 输出](/docs/how_to/output_parser_yaml)
- [如何：在输出解析错误时重试](/docs/how_to/output_parser_retry)
- [如何：尝试修复输出解析中的错误](/docs/how_to/output_parser_fixing)
- [如何：编写自定义输出解析器类](/docs/how_to/output_parser_custom)

### 文档加载器

[文档加载器](/docs/concepts/document_loaders) 负责从各种来源加载文档。

- [如何：加载 PDF 文件](/docs/how_to/document_loader_pdf)
- [如何：加载网页](/docs/how_to/document_loader_web)
- [如何：加载 CSV 数据](/docs/how_to/document_loader_csv)
- [如何：从目录加载数据](/docs/how_to/document_loader_directory)
- [如何：加载 HTML 数据](/docs/how_to/document_loader_html)
- [如何：加载 JSON 数据](/docs/how_to/document_loader_json)
- [如何：加载 Markdown 数据](/docs/how_to/document_loader_markdown)
- [如何：加载 Microsoft Office 数据](/docs/how_to/document_loader_office_file)
- [如何：编写自定义文档加载器](/docs/how_to/document_loader_custom)

### 文本分割器

[文本分割器](/docs/concepts/text_splitters) 将文档分割成可用于检索的块。

- [如何：递归分割文本](/docs/how_to/recursive_text_splitter)
- [如何：分割 HTML](/docs/how_to/split_html)
- [如何：按字符分割](/docs/how_to/character_text_splitter)
- [如何：分割代码](/docs/how_to/code_splitter)
- [如何：按标题分割 Markdown](/docs/how_to/markdown_header_metadata_splitter)
- [如何：递归分割 JSON](/docs/how_to/recursive_json_splitter)
- [如何：将文本分割为语义块](/docs/how_to/semantic-chunker)
- [如何：按令牌分割](/docs/how_to/split_by_token)

### 嵌入模型

[嵌入模型](/docs/concepts/embedding_models) 接收一段文本并创建其数值表示。
有关从特定提供商开始使用嵌入模型的详细信息，请参阅[支持的集成](/docs/integrations/text_embedding/)。

- [如何：嵌入文本数据](/docs/how_to/embed_text)
- [如何：缓存嵌入结果](/docs/how_to/caching_embeddings)
- [如何：创建自定义嵌入类](/docs/how_to/custom_embeddings)

### 向量存储

[向量存储](/docs/concepts/vectorstores) 是可以高效存储和检索嵌入的数据库。
有关从特定提供商开始使用向量存储的详细信息，请参阅[支持的集成](/docs/integrations/vectorstores/)。

- [如何：使用向量存储检索数据](/docs/how_to/vectorstores)

### 检索器

[检索器](/docs/concepts/retrievers) 负责接收查询并返回相关文档。

- [如何：使用向量存储检索数据](/docs/how_to/vectorstore_retriever)
- [如何：生成多个查询以检索数据](/docs/how_to/MultiQueryRetriever)
- [如何：使用上下文压缩压缩检索到的数据](/docs/how_to/contextual_compression)
- [如何：编写自定义检索器类](/docs/how_to/custom_retriever)
- [如何：为检索结果添加相似性分数](/docs/how_to/add_scores_retriever)
- [如何：结合多个检索器的结果](/docs/how_to/ensemble_retriever)
- [如何：重新排序检索到的结果以减轻“中间丢失”效应](/docs/how_to/long_context_reorder)
- [如何：为文档生成多个嵌入](/docs/how_to/multi_vector)
- [如何：为块检索整个文档](/docs/how_to/parent_document_retriever)
- [如何：生成元数据过滤器](/docs/how_to/self_query)
- [如何：创建时间加权检索器](/docs/how_to/time_weighted_vectorstore)
- [如何：使用混合向量和关键字检索](/docs/how_to/hybrid)

### 索引

索引是使向量存储与基础数据源保持同步的过程。

- [如何：重新索引数据以使向量存储与基础数据源保持同步](/docs/how_to/indexing)

### 工具

LangChain [工具](/docs/concepts/tools) 包含工具的描述（传递给语言模型）以及要调用的函数的实现。有关预构建工具的列表，请参阅[此处](/docs/integrations/tools/)。

- [如何：创建工具](/docs/how_to/custom_tools)
- [如何：使用内置工具和工具包](/docs/how_to/tools_builtin)
- [如何：使用聊天模型调用工具](/docs/how_to/tool_calling)
- [如何：将工具输出传递给聊天模型](/docs/how_to/tool_results_pass_to_model)
- [如何：将运行时值传递给工具](/docs/how_to/tool_runtime)
- [如何：为工具添加人工干预](/docs/how_to/tools_human)
- [如何：处理工具错误](/docs/how_to/tools_error)
- [如何：强制模型调用工具](/docs/how_to/tool_choice)
- [如何：禁用并行工具调用](/docs/how_to/tool_calling_parallel)
- [如何：从工具访问 `RunnableConfig`](/docs/how_to/tool_configure)
- [如何：从工具流式事件](/docs/how_to/tool_stream_events)
- [如何：从工具返回工件](/docs/how_to/tool_artifacts/)
- [如何：将 Runnables 转换为工具](/docs/how_to/convert_runnable_to_tool)
- [如何：为模型添加临时工具调用功能](/docs/how_to/tools_prompting)
- [如何：传递运行时机密](/docs/how_to/runnable_runtime_secrets)

### 多模态

- [如何：将多模态数据直接传递给模型](/docs/how_to/multimodal_inputs/)
- [如何：使用多模态提示](/docs/how_to/multimodal_prompts/)

### 代理

:::note

有关代理的深入操作指南，请查看 [LangGraph](https://langchain-ai.github.io/langgraph/) 文档。

:::

- [如何：使用传统 LangChain 代理 (AgentExecutor)](/docs/how_to/agent_executor)
- [如何：从传统 LangChain 代理迁移到 LangGraph](/docs/how_to/migrate_agent)

### 回调

[回调](/docs/concepts/callbacks) 允许您挂钩到 LLM 应用程序执行的各个阶段。

- [如何：在运行时传递回调](/docs/how_to/callbacks_runtime)
- [如何：将回调附加到模块](/docs/how_to/callbacks_attach)
- [如何：将回调传递到模块构造函数](/docs/how_to/callbacks_constructor)
- [如何：创建自定义回调处理程序](/docs/how_to/custom_callbacks)
- [如何：在异步环境中使用回调](/docs/how_to/callbacks_async)
- [如何：调度自定义回调事件](/docs/how_to/callbacks_custom_events)

### 自定义

LangChain 的所有组件都可以轻松扩展以支持您自己的版本。

- [如何：创建自定义聊天模型类](/docs/how_to/custom_chat_model)
- [如何：创建自定义 LLM 类](/docs/how_to/custom_llm)
- [如何：创建自定义嵌入类](/docs/how_to/custom_embeddings)
- [如何：编写自定义检索器类](/docs/how_to/custom_retriever)
- [如何：编写自定义文档加载器](/docs/how_to/document_loader_custom)
- [如何：编写自定义输出解析器类](/docs/how_to/output_parser_custom)
- [如何：创建自定义回调处理程序](/docs/how_to/custom_callbacks)
- [如何：定义自定义工具](/docs/how_to/custom_tools)
- [如何：调度自定义回调事件](/docs/how_to/callbacks_custom_events)

### 序列化
- [如何：保存和加载 LangChain 对象](/docs/how_to/serialization)

## 使用案例

这些指南涵盖了特定使用案例的详细信息。

### 使用 RAG 进行问答

检索增强生成 (RAG) 是一种将 LLM 连接到外部数据源的方法。
有关 RAG 的高级教程，请查看[此指南](/docs/tutorials/rag/)。

- [如何：添加聊天历史记录](/docs/how_to/qa_chat_history_how_to/)
- [如何：流式传输](/docs/how_to/qa_streaming/)
- [如何：返回来源](/docs/how_to/qa_sources/)
- [如何：返回引用](/docs/how_to/qa_citations/)
- [如何：进行每用户检索](/docs/how_to/qa_per_user/)

### 提取

提取是指使用 LLM 从非结构化文本中提取结构化信息。
有关提取的高级教程，请查看[此指南](/docs/tutorials/extraction/)。

- [如何：使用参考示例](/docs/how_to/extraction_examples/)
- [如何：处理长文本](/docs/how_to/extraction_long_text/)
- [如何：在不使用函数调用的情况下进行提取](/docs/how_to/extraction_parse)

### 聊天机器人

聊天机器人涉及使用 LLM 进行对话。
有关构建聊天机器人的高级教程，请查看[此指南](/docs/tutorials/chatbot/)。

- [如何：管理内存](/docs/how_to/chatbots_memory)
- [如何：进行检索](/docs/how_to/chatbots_retrieval)
- [如何：使用工具](/docs/how_to/chatbots_tools)
- [如何：管理大型聊天历史记录](/docs/how_to/trim_messages/)

### 查询分析

查询分析是使用 LLM 生成要发送到检索器的查询的任务。
有关查询分析的高级教程，请查看[此指南](/docs/tutorials/rag/#query-analysis)。

- [如何：向提示添加示例](/docs/how_to/query_few_shot)
- [如何：处理未生成查询的情况](/docs/how_to/query_no_queries)
- [如何：处理多个查询](/docs/how_to/query_multiple_queries)
- [如何：处理多个检索器](/docs/how_to/query_multiple_retrievers)
- [如何：构建过滤器](/docs/how_to/query_constructing_filters)
- [如何：处理高基数分类变量](/docs/how_to/query_high_cardinality)

### SQL + CSV 问答

您可以使用 LLM 对表格数据进行问答。
有关高级教程，请查看[此指南](/docs/tutorials/sql_qa/)。

- [如何：使用提示改善结果](/docs/how_to/sql_prompting)
- [如何：进行查询验证](/docs/how_to/sql_query_checking)
- [如何：处理大型数据库](/docs/how_to/sql_large_db)
- [如何：处理 CSV 文件](/docs/how_to/sql_csv)

### 图数据库问答

您可以使用 LLM 对图数据库进行问答。
有关高级教程，请查看[此指南](/docs/tutorials/graph/)。

- [如何：在数据库上添加语义层](/docs/how_to/graph_semantic)
- [如何：构建知识图谱](/docs/how_to/graph_constructing)

### 摘要

LLM 可以总结并以其他方式提取文本中的所需信息，包括大量文本。
有关高级教程，请查看[此指南](/docs/tutorials/summarization)。

- [如何：在单次 LLM 调用中总结文本](/docs/how_to/summarize_stuff)
- [如何：通过并行化总结文本](/docs/how_to/summarize_map_reduce)
- [如何：通过迭代优化总结文本](/docs/how_to/summarize_refine)

## LangChain 表达语言 (LCEL)

:::note 我应该使用 LCEL 吗？

LCEL 是一种编排解决方案。请参阅我们的
[概念页面](/docs/concepts/lcel/#should-i-use-lcel) 了解何时使用 LCEL 的建议。

:::

[LangChain 表达语言](/docs/concepts/lcel) 是一种创建任意自定义链的方法。它基于 [Runnable](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html) 协议。

[**LCEL 速查表**](/docs/how_to/lcel_cheatsheet/)：快速概览如何使用主要 LCEL 原语。

[**迁移指南**](/docs/versions/migrating_chains)：将传统链抽象迁移到 LCEL。

- [如何：链式运行](/docs/how_to/sequence)
- [如何：流式运行](/docs/how_to/streaming)
- [如何：并行调用](/docs/how_to/parallel/)
- [如何：为运行添加默认调用参数](/docs/how_to/binding/)
- [如何：将任何函数转换为运行](/docs/how_to/functions)
- [如何：从一个链步骤传递输入到下一个](/docs/how_to/passthrough)
- [如何：在运行时配置运行行为](/docs/how_to/configure)
- [如何：向链添加消息历史记录（内存）](/docs/how_to/message_history)
- [如何：在子链之间路由](/docs/how_to/routing)
- [如何：创建动态（自构建）链](/docs/how_to/dynamic_chain/)
- [如何：检查运行](/docs/how_to/inspect)
- [如何：为运行添加回退](/docs/how_to/fallbacks)
- [如何：将运行时机密传递给运行](/docs/how_to/runnable_runtime_secrets)

## [LangGraph](https://langchain-ai.github.io/langgraph)

LangGraph 是 LangChain 的扩展，旨在通过将步骤建模为图中的边和节点来构建具有 LLM 的强大且有状态的多参与者应用程序。

LangGraph 文档目前托管在单独的网站上。
您可以浏览 [LangGraph 操作指南](https://langchain-ai.github.io/langgraph/how-tos/)。

## [LangSmith](https://docs.smith.langchain.com/)

LangSmith 允许您密切跟踪、监控和评估您的 LLM 应用程序。
它与 LangChain 和 LangGraph 无缝集成，您可以使用它在构建时检查和调试链和代理的各个步骤。

LangSmith 文档托管在单独的网站上。
您可以浏览 [LangSmith 操作指南](https://docs.smith.langchain.com/)，但我们将在下面重点介绍一些与 LangChain 特别相关的部分：

### 评估
<span data-heading-keywords="evaluation,evaluate"></span>

评估性能是构建 LLM 驱动应用程序的重要部分。
LangSmith 从创建数据集到定义指标再到运行评估器的每一步都提供帮助。

要了解更多信息，请查看 [LangSmith 评估操作指南](https://docs.smith.langchain.com/how_to_guides#evaluation)。

### 跟踪
<span data-heading-keywords="trace,tracing"></span>

跟踪为您的链和代理提供可观察性，并且在诊断问题时至关重要。

- [如何：使用 LangChain 进行跟踪](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)
- [如何：向跟踪添加元数据和标签](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces)

您可以在 [LangSmith 文档的此部分](https://docs.smith.langchain.com/how_to_guides/tracing) 中查看与跟踪相关的一般操作指南。
