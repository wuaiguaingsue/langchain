{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何从工具流式传输事件\n",
    "\n",
    ":::info 前置条件\n",
    "\n",
    "本指南假设您已熟悉以下概念：\n",
    "- [LangChain 工具](/docs/concepts/tools)\n",
    "- [自定义工具](/docs/how_to/custom_tools)\n",
    "- [使用流事件](/docs/how_to/streaming/#using-stream-events)\n",
    "- [在自定义工具中访问 RunnableConfig](/docs/how_to/tool_configure/)\n",
    "\n",
    ":::\n",
    "\n",
    "如果您有调用 [聊天模型](/docs/concepts/chat_models/)、[检索器](/docs/concepts/retrievers/)或其他 [runnables](/docs/concepts/runnables/) 的 [工具](/docs/concepts/tools/)，您可能希望访问这些 runnables 的内部事件或使用其他属性配置它们。本指南向您展示如何正确手动传递参数，以便使用 `astream_events()` 方法实现此目的。\n",
    "\n",
    ":::caution 兼容性\n",
    "\n",
    "如果您在 `python<=3.10` 中运行 `async` 代码，LangChain 无法自动将配置（包括 `astream_events()` 所需的回调）传播到子 runnable。这是您可能无法看到自定义 runnable 或工具发出事件的常见原因。\n",
    "\n",
    "如果您运行的是 `python<=3.10`，则需要在异步环境中手动将 `RunnableConfig` 对象传播到子 runnable。有关如何手动传播配置的示例，请参阅下面 `bar` RunnableLambda 的实现。\n",
    "\n",
    "如果您运行的是 `python>=3.11`，则 `RunnableConfig` 将在异步环境中自动传播到子 runnable。但是，如果您的代码可能在旧版 Python 中运行，手动传播 `RunnableConfig` 仍然是一个好主意。\n",
    "\n",
    "本指南还需要 `langchain-core>=0.2.16`。\n",
    ":::\n",
    "\n",
    "假设您有一个自定义工具，它通过提示聊天模型返回仅 10 个单词的输入摘要，然后反转输出。首先，以一种简单的方式定义它：\n",
    "\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs customVarName=\"model\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "if \"ANTHROPIC_API_KEY\" not in os.environ:\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = getpass()\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "async def special_summarization_tool(long_text: str) -> str:\n",
    "    \"\"\"使用高级技术总结输入文本的工具。\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"您是一位专家作家。用 10 个单词或更少总结以下文本：\\n\\n{long_text}\"\n",
    "    )\n",
    "\n",
    "    def reverse(x: str):\n",
    "        return x[::-1]\n",
    "\n",
    "    chain = prompt | model | StrOutputParser() | reverse\n",
    "    summary = await chain.ainvoke({\"long_text\": long_text})\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接调用工具可以正常工作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.yad noitaudarg rof tiftuo sesoohc yrraB ;scisyhp seifed eeB'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LONG_TEXT = \"\"\"\n",
    "旁白：\n",
    "(黑屏显示文字；可以听到蜜蜂嗡嗡的声音)\n",
    "根据所有已知的航空法则，蜜蜂不可能飞行。它的翅膀太小，无法让它胖胖的小身体离开地面。当然，蜜蜂还是飞了，因为蜜蜂不在乎人类认为不可能的事情。\n",
    "BARRY BENSON：\n",
    "(Barry 正在挑选一件衬衫)\n",
    "黄色，黑色。黄色，黑色。黄色，黑色。黄色，黑色。哦，黑色和黄色！让我们稍微改变一下。\n",
    "JANET BENSON：\n",
    "Barry！早餐准备好了！\n",
    "BARRY：\n",
    "来了！稍等一下。\n",
    "\"\"\"\n",
    "\n",
    "await special_summarization_tool.ainvoke({\"long_text\": LONG_TEXT})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是，如果您想访问聊天模型的原始输出而不是完整工具，您可能会尝试使用 [`astream_events()`](/docs/how_to/streaming/#using-stream-events) 方法并查找 `on_chat_model_end` 事件。以下是发生的情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = special_summarization_tool.astream_events({\"long_text\": LONG_TEXT})\n",
    "\n",
    "async for event in stream:\n",
    "    if event[\"event\"] == \"on_chat_model_end\":\n",
    "        # 在 python<=3.10 中永远不会触发！\n",
    "        print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您会注意到（除非您在 `python>=3.11` 中运行本指南），没有从子运行中发出的聊天模型事件！\n",
    "\n",
    "这是因为上面的示例没有将工具的配置对象传递到内部链中。要解决此问题，请重新定义您的工具以接受一个特殊参数，其类型为 `RunnableConfig`（有关更多详细信息，请参阅 [本指南](/docs/how_to/tool_configure)）。您还需要在执行内部链时传递该参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "@tool\n",
    "async def special_summarization_tool_with_config(\n",
    "    long_text: str, config: RunnableConfig\n",
    ") -> str:\n",
    "    \"\"\"使用高级技术总结输入文本的工具。\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"您是一位专家作家。用 10 个单词或更少总结以下文本：\\n\\n{long_text}\"\n",
    "    )\n",
    "\n",
    "    def reverse(x: str):\n",
    "        return x[::-1]\n",
    "\n",
    "    chain = prompt | model | StrOutputParser() | reverse\n",
    "    # 将 \"config\" 对象作为参数传递给任何执行的 runnables\n",
    "    summary = await chain.ainvoke({\"long_text\": long_text}, config=config)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在尝试使用新工具执行与之前相同的 `astream_events()` 调用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chat_model_end', 'data': {'output': AIMessage(content='Bee defies physics; Barry chooses outfit for graduation day.', additional_kwargs={}, response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run-337ac14e-8da8-4c6d-a69f-1573f93b651e', usage_metadata={'input_tokens': 182, 'output_tokens': 19, 'total_tokens': 201, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}), 'input': {'messages': [[HumanMessage(content=\"You are an expert writer. Summarize the following text in 10 words or less:\\n\\n\\nNARRATOR:\\n(Black screen with text; The sound of buzzing bees can be heard)\\nAccording to all known laws of aviation, there is no way a bee should be able to fly. Its wings are too small to get its fat little body off the ground. The bee, of course, flies anyway because bees don't care what humans think is impossible.\\nBARRY BENSON:\\n(Barry is picking out a shirt)\\nYellow, black. Yellow, black. Yellow, black. Yellow, black. Ooh, black and yellow! Let's shake it up a little.\\nJANET BENSON:\\nBarry! Breakfast is ready!\\nBARRY:\\nComing! Hang on a second.\\n\", additional_kwargs={}, response_metadata={})]]}}, 'run_id': '337ac14e-8da8-4c6d-a69f-1573f93b651e', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['225beaa6-af73-4c91-b2d3-1afbbb88d53e']}\n"
     ]
    }
   ],
   "source": [
    "stream = special_summarization_tool_with_config.astream_events({\"long_text\": LONG_TEXT})\n",
    "\n",
    "async for event in stream:\n",
    "    if event[\"event\"] == \"on_chat_model_end\":\n",
    "        print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "太棒了！这次发出了一个事件。\n",
    "\n",
    "对于流式处理，`astream_events()` 会在可能的情况下自动调用链中的内部 runnables 并启用流式处理，因此如果您想要从聊天模型生成的令牌流，您只需过滤以查找 `on_chat_model_stream` 事件，而无需其他更改："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', usage_metadata={'input_tokens': 182, 'output_tokens': 2, 'total_tokens': 184, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='Bee', additional_kwargs={}, response_metadata={}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5')}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' defies physics;', additional_kwargs={}, response_metadata={}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5')}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' Barry chooses outfit for', additional_kwargs={}, response_metadata={}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5')}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' graduation day.', additional_kwargs={}, response_metadata={}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5')}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', usage_metadata={'input_tokens': 0, 'output_tokens': 17, 'total_tokens': 17, 'input_token_details': {}})}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}\n"
     ]
    }
   ],
   "source": [
    "stream = special_summarization_tool_with_config.astream_events({\"long_text\": LONG_TEXT})\n",
    "\n",
    "async for event in stream:\n",
    "    if event[\"event\"] == \"on_chat_model_stream\":\n",
    "        print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下一步\n",
    "\n",
    "您现在已经了解了如何从工具中流式传输事件。接下来，请查看以下指南以了解有关使用工具的更多信息：\n",
    "\n",
    "- 传递 [运行时值到工具](/docs/how_to/tool_runtime)\n",
    "- 将 [工具结果传递回模型](/docs/how_to/tool_results_pass_to_model)\n",
    "- [分派自定义回调事件](/docs/how_to/callbacks_custom_events)\n",
    "\n",
    "您还可以查看一些更具体的工具调用用途：\n",
    "\n",
    "- 构建 [使用工具的链和代理](/docs/how_to#tools)\n",
    "- 从模型获取 [结构化输出](/docs/how_to/structured_output/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
