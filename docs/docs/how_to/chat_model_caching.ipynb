{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcf87b32",
   "metadata": {},
   "source": [
    "# 如何缓存聊天模型的响应\n",
    "\n",
    ":::info 前置条件\n",
    "\n",
    "本指南假设您已熟悉以下概念：\n",
    "- [聊天模型](/docs/concepts/chat_models)\n",
    "- [LLMs](/docs/concepts/text_llms)\n",
    "\n",
    ":::\n",
    "\n",
    "LangChain 为 [聊天模型](/docs/concepts/chat_models) 提供了一个可选的缓存层。这主要有两个好处：\n",
    "\n",
    "- 如果您经常多次请求相同的完成内容，它可以通过减少对 LLM 提供商的 API 调用次数来为您节省费用。这在应用程序开发期间尤其有用。\n",
    "- 它可以通过减少对 LLM 提供商的 API 调用次数来加快您的应用程序速度。\n",
    "\n",
    "本指南将向您展示如何在您的应用程序中启用此功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289b31de",
   "metadata": {},
   "source": [
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs customVarName=\"llm\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6641f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass()\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5472a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <!-- ruff: noqa: F821 -->\n",
    "from langchain_core.globals import set_llm_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357b89a8",
   "metadata": {},
   "source": [
    "## 内存缓存\n",
    "\n",
    "这是一个临时缓存，将模型调用存储在内存中。当您的环境重启时，它将被清除，并且不会在进程之间共享。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e719a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 645 ms, sys: 214 ms, total: 859 ms\n",
      "Wall time: 829 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\", response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 11, 'total_tokens': 24}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-b6836bdd-8c30-436b-828f-0ac5fc9ab50e-0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_core.caches import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# 第一次调用时，尚未缓存，因此会花费更长时间\n",
    "llm.invoke(\"讲个笑话\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2121434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 822 µs, sys: 288 µs, total: 1.11 ms\n",
      "Wall time: 1.06 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\", response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 11, 'total_tokens': 24}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-b6836bdd-8c30-436b-828f-0ac5fc9ab50e-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 第二次调用时，已缓存，因此速度更快\n",
    "llm.invoke(\"讲个笑话\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88ff8af",
   "metadata": {},
   "source": [
    "## SQLite 缓存\n",
    "\n",
    "此缓存实现使用 `SQLite` 数据库存储响应，并且在进程重启后仍然有效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99290ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm .langchain.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe826c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们可以对 SQLite 缓存执行相同的操作\n",
    "from langchain_community.cache import SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb558734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.91 ms, sys: 7.68 ms, total: 17.6 ms\n",
      "Wall time: 657 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the scarecrow win an award? Because he was outstanding in his field!', response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 11, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-39d9e1e8-7766-4970-b1d8-f50213fd94c5-0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 第一次调用时，尚未缓存，因此会花费更长时间\n",
    "llm.invoke(\"讲个笑话\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c7000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.2 ms, sys: 60.5 ms, total: 113 ms\n",
      "Wall time: 127 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the scarecrow win an award? Because he was outstanding in his field!', id='run-39d9e1e8-7766-4970-b1d8-f50213fd94c5-0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 第二次调用时，已缓存，因此速度更快\n",
    "llm.invoke(\"讲个笑话\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2950a913",
   "metadata": {},
   "source": [
    "## 下一步\n",
    "\n",
    "您现在已经了解了如何缓存模型响应以节省时间和金钱。\n",
    "\n",
    "接下来，请查看本节中关于聊天模型的其他操作指南，例如[如何让模型返回结构化输出](/docs/how_to/structured_output)或[如何创建您自己的自定义聊天模型](/docs/how_to/custom_chat_model)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
