{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e9437c8a-d8b7-4bf6-8ff4-54068a5a266c",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 1.5\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df7646-b1e1-4014-a841-6dae9b3c50d9",
   "metadata": {},
   "source": [
    "# 如何流式传输聊天模型响应\n",
    "\n",
    "\n",
    "所有[聊天模型](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html)都实现了[Runnable 接口](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)，该接口附带了标准可运行方法（即`ainvoke`、`batch`、`abatch`、`stream`、`astream`、`astream_events`）的**默认**实现。\n",
    "\n",
    "**默认**流式传输实现提供了一个`Iterator`（或用于异步流式传输的`AsyncIterator`），它仅生成一个值：来自底层聊天模型提供者的最终输出。\n",
    "\n",
    ":::提示\n",
    "\n",
    "**默认**实现**不**支持逐个令牌的流式传输，但它确保模型可以替换为任何其他模型，因为它支持相同的标准接口。\n",
    "\n",
    ":::\n",
    "\n",
    "逐个令牌流式传输的能力取决于提供者是否实现了适当的流式传输支持。\n",
    "\n",
    "查看哪些[集成支持逐个令牌流式传输](/docs/integrations/chat/)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a76660e-7691-48b7-a2b4-2ccdff7875c3",
   "metadata": {},
   "source": [
    "## 同步流式传输\n",
    "\n",
    "下面我们使用`|`来帮助可视化令牌之间的分隔符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975c4f32-21f6-4a71-9091-f87b56347c33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here| is| a| |1| |verse| song| about| gol|dfish| on| the| moon|:|\n",
      "\n",
      "Floating| up| in| the| star|ry| night|,|\n",
      "Fins| a|-|gl|im|mer| in| the| pale| moon|light|.|\n",
      "Gol|dfish| swimming|,| peaceful| an|d free|,|\n",
      "Se|ren|ely| |drif|ting| across| the| lunar| sea|.|"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic.chat_models import ChatAnthropic\n",
    "\n",
    "chat = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "for chunk in chat.stream(\"为我写一首关于月球上的金鱼的单节歌曲\"):\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482d3a7-ee4f-40ba-b871-4d3f52603cd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 异步流式传输"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422f480c-df79-42e8-9bee-d0ebed31c557",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here| is| a| |1| |verse| song| about| gol|dfish| on| the| moon|:|\n",
      "\n",
      "Floating| up| above| the| Earth|,|\n",
      "Gol|dfish| swim| in| alien| m|irth|.|\n",
      "In| their| bowl| of| lunar| dust|,|\n",
      "Gl|it|tering| scales| reflect| the| trust|\n",
      "Of| swimming| free| in| this| new| worl|d,|\n",
      "Where| their| aqu|atic| dream|'s| unf|ur|le|d.|"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic.chat_models import ChatAnthropic\n",
    "\n",
    "chat = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "async for chunk in chat.astream(\"为我写一首关于月球上的金鱼的单节歌曲\"):\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e1309-3b6e-42fb-820a-2e4e3e6bc074",
   "metadata": {},
   "source": [
    "## Astream 事件\n",
    "\n",
    "聊天模型还支持标准的[astream 事件](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream_events)方法。\n",
    "\n",
    "如果您正在从包含多个步骤的更大 LLM 应用程序（例如由提示、LLM 和解析器组成的 LLM 链）中流式传输输出，此方法非常有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a03086e-2813-4cb1-b12b-d00e7eeba122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chat_model_start', 'data': {'input': 'Write me a 1 verse song about goldfish on the moon'}, 'name': 'ChatAnthropic', 'tags': [], 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'name': 'ChatAnthropic', 'tags': [], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-1d430164-52b1-4d00-8c00-b16460f7737e', usage_metadata={'input_tokens': 21, 'output_tokens': 2, 'total_tokens': 23, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'name': 'ChatAnthropic', 'tags': [], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content=\"Here's\", additional_kwargs={}, response_metadata={}, id='run-1d430164-52b1-4d00-8c00-b16460f7737e')}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'name': 'ChatAnthropic', 'tags': [], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content=' a short one-verse song', additional_kwargs={}, response_metadata={}, id='run-1d430164-52b1-4d00-8c00-b16460f7737e')}, 'parent_ids': []}\n",
      "...Truncated\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic.chat_models import ChatAnthropic\n",
    "\n",
    "chat = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "idx = 0\n",
    "\n",
    "async for event in chat.astream_events(\n",
    "    \"为我写一首关于月球上的金鱼的单节歌曲\"\n",
    "):\n",
    "    idx += 1\n",
    "    if idx >= 5:  # 截断输出\n",
    "        print(\"...已截断\")\n",
    "        break\n",
    "    print(event)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
