{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fc3897d-176f-4729-8fd1-cfb4add53abd",
   "metadata": {},
   "source": [
    "## Nomic 多模态 RAG\n",
    "\n",
    "许多文档包含混合内容类型,包括文本和图像。\n",
    "\n",
    "然而,在大多数 RAG 应用程序中,图像中包含的信息都被丢失了。\n",
    "\n",
    "随着多模态 LLM 的出现,例如 [GPT-4V](https://openai.com/research/gpt-4v-system-card),值得考虑如何在 RAG 中利用图像:\n",
    "\n",
    "在这个演示中我们:\n",
    "\n",
    "* 使用来自 Nomic Embed 的多模态嵌入模型[Vision](https://huggingface.co/nomic-ai/nomic-embed-vision-v1.5)和[Text](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5)来嵌入图像和文本\n",
    "* 使用相似度搜索来检索两者\n",
    "* 将原始图像和文本块传递给多模态 LLM 进行答案合成\n",
    "\n",
    "## 注册\n",
    "\n",
    "获取 API 令牌,然后运行:\n",
    "```\n",
    "! nomic login\n",
    "```\n",
    "\n",
    "然后使用生成的 API 令牌运行\n",
    "```\n",
    "! nomic login < token >\n",
    "```\n",
    "\n",
    "## 包\n",
    "\n",
    "对于 `unstructured`,您还需要在系统中安装 `poppler`([安装说明](https://pdf2image.readthedocs.io/en/latest/installation.html))和 `tesseract`([安装说明](https://tesseract-ocr.github.io/tessdoc/Installation.html))。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54926b9b-75c2-4cd4-8f14-b3882a0d370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nomic login token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febbc459-ebba-4c1a-a52b-fed7731593f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install -U langchain-nomic langchain-chroma langchain-community tiktoken langchain-openai langchain # (newest versions required for multi-modal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbdc603-39e2-4a5f-836c-2bbaecd46b0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lock to 0.10.19 due to a persistent bug in more recent versions\n",
    "! pip install \"unstructured[all-docs]==0.10.19\" pillow pydantic lxml pillow matplotlib tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e94b3fb-8e3e-4736-be0a-ad881626c7bd",
   "metadata": {},
   "source": [
    "## 数据加载\n",
    "\n",
    "### 分割 PDF 文本和图像\n",
    "  \n",
    "让我们看一个包含有趣图像的 PDF 示例。\n",
    "\n",
    "1/ 来自 J Paul Getty 博物馆的艺术品:\n",
    "\n",
    " * 这里有一个 [zip 文件](https://drive.google.com/file/d/18kRKbq2dqAhhJ3DfZRnYcTBEUfYxe1YR/view?usp=sharing),其中包含 PDF 和已提取的图像。\n",
    "* https://www.getty.edu/publications/resources/virtuallibrary/0892360224.pdf\n",
    "\n",
    "2/ 来自国会图书馆的著名照片:\n",
    "\n",
    "* https://www.loc.gov/lcm/pdf/LCM_2020_1112.pdf\n",
    "* 我们将在下面使用这个作为示例\n",
    "\n",
    "我们可以使用 [Unstructured](https://unstructured-io.github.io/unstructured/introduction.html#key-concepts) 中的 `partition_pdf` 来提取文本和图像。\n",
    "\n",
    "要提取图像,请提供以下参数:\n",
    "```\n",
    "extract_images_in_pdf=True\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "如果使用此 zip 文件,则可以仅处理文本:\n",
    "```\n",
    "extract_images_in_pdf=False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9646b524-71a7-4b2a-bdc8-0b81f77e968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 包含 PDF 和提取图像的文件夹\n",
    "from pathlib import Path\n",
    "\n",
    "# 替换为实际图像路径\n",
    "path = Path(\"../art\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f096ab-a933-41d0-8f4e-1efc83998fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path.resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4839c0-8773-4a07-ba59-5364501269b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取图像、表格和分块文本\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=str(path.resolve()) + \"/getty.pdf\",\n",
    "    extract_images_in_pdf=False,\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969545ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按类型分类文本元素\n",
    "tables = []\n",
    "texts = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        tables.append(str(element))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        texts.append(str(element))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8e6349-1547-4cbf-9c6f-491d8610ec10",
   "metadata": {},
   "source": [
    "## 使用我们的文档进行多模态嵌入\n",
    "\n",
    "我们将使用 [nomic-embed-vision-v1.5](https://huggingface.co/nomic-ai/nomic-embed-vision-v1.5) 嵌入模型。此模型与 [nomic-embed-text-v1.5](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5) 对齐,允许进行多模态语义搜索和多模态 RAG!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc15842-cb95-4f84-9eb5-656b0282a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "\n",
    "import chromadb\n",
    "import numpy as np\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_nomic import NomicEmbeddings\n",
    "from PIL import Image as _PILImage\n",
    "\n",
    "# 创建 chroma\n",
    "text_vectorstore = Chroma(\n",
    "    collection_name=\"mm_rag_clip_photos_text\",\n",
    "    embedding_function=NomicEmbeddings(\n",
    "        vision_model=\"nomic-embed-vision-v1.5\", model=\"nomic-embed-text-v1.5\"\n",
    "    ),\n",
    ")\n",
    "image_vectorstore = Chroma(\n",
    "    collection_name=\"mm_rag_clip_photos_image\",\n",
    "    embedding_function=NomicEmbeddings(\n",
    "        vision_model=\"nomic-embed-vision-v1.5\", model=\"nomic-embed-text-v1.5\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 获取仅带有 .jpg 扩展名的图像 URI\n",
    "image_uris = sorted(\n",
    "    [\n",
    "        os.path.join(path, image_name)\n",
    "        for image_name in os.listdir(path)\n",
    "        if image_name.endswith(\".jpg\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 添加图像\n",
    "image_vectorstore.add_images(uris=image_uris)\n",
    "\n",
    "# 添加文档\n",
    "text_vectorstore.add_texts(texts=texts)\n",
    "\n",
    "# 创建检索器\n",
    "image_retriever = image_vectorstore.as_retriever()\n",
    "text_retriever = text_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a186d0-27e0-4820-8092-63b5349dd25d",
   "metadata": {},
   "source": [
    "## RAG\n",
    "\n",
    "`vectorstore.add_images` 将以 base64 编码字符串的形式存储/检索图像。\n",
    "\n",
    "这些可以传递给 [GPT-4V](https://platform.openai.com/docs/guides/vision)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f56a8-0dc3-433e-851c-3f7600c7a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    调整以 Base64 字符串编码的图像大小。\n",
    "\n",
    "    参数:\n",
    "    base64_string (str): 原始图像的 Base64 字符串。\n",
    "    size (tuple): 图像的目标大小,格式为 (宽度, 高度)。\n",
    "\n",
    "    返回:\n",
    "    str: 调整大小后的图像的 Base64 字符串。\n",
    "    \"\"\"\n",
    "    # 解码 Base64 字符串\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # 调整图像大小\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # 将调整大小后的图像保存到字节缓冲区\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # 将调整大小后的图像编码为 Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def is_base64(s):\n",
    "    \"\"\"检查字符串是否为 Base64 编码\"\"\"\n",
    "    try:\n",
    "        return base64.b64encode(base64.b64decode(s)) == s.encode()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"分割 numpy 数组图像和文本\"\"\"\n",
    "    images = []\n",
    "    text = []\n",
    "    for doc in docs:\n",
    "        doc = doc.page_content  # 提取文档内容\n",
    "        if is_base64(doc):\n",
    "            # 调整图像大小以避免 OAI 服务器错误\n",
    "            images.append(\n",
    "                resize_base64_image(doc, size=(250, 250))\n",
    "            )  # base64 编码字符串\n",
    "        else:\n",
    "            text.append(doc)\n",
    "    return {\"images\": images, \"texts\": text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a2c1d8-fea6-4152-b184-3172dd46c735",
   "metadata": {},
   "source": [
    "目前,我们使用 `RunnableLambda` 格式化输入,同时我们将图像支持添加到 `ChatPromptTemplates`。\n",
    "\n",
    "我们的 runnable 遵循经典的 RAG 流程 - \n",
    "\n",
    "* 我们首先计算上下文(在这种情况下是 \"texts\" 和 \"images\")以及问题(这里只是一个 RunnablePassthrough)\n",
    "* 然后我们将其传递到我们的提示模板中,这是一个自定义函数,用于格式化 gpt-4-vision-preview 模型的消息。\n",
    "* 最后我们将输出解析为字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8919dc-c238-4746-86ba-45d940a7d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c93fab3-74c4-4f1d-958a-0bc4cdd0797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def prompt_func(data_dict):\n",
    "    # 将上下文文本连接成一个字符串\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"text_context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # 如果存在图像,将其添加到消息中\n",
    "    if data_dict[\"image_context\"][\"images\"]:\n",
    "        image_message = {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{data_dict['image_context']['images'][0]}\"\n",
    "            },\n",
    "        }\n",
    "        messages.append(image_message)\n",
    "\n",
    "    # 添加文本消息进行分析\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"作为一名专家艺术评论家和历史学家,您的任务是分析和解释图像, \"\n",
    "            \"考虑其历史和文化意义。除了图像外,您还将获得相关文本以提供上下文。两者都将从向量存储中检索, \"\n",
    "            \"基于用户输入的关键字。请使用您的广泛知识和分析技能提供一个全面的总结,包括:\\n\"\n",
    "            \"- 图像中的视觉元素的详细描述。\\n\"\n",
    "            \"- 图像的历史和文化背景。\\n\"\n",
    "            \"- 图像的象征意义和含义的解释。\\n\"\n",
    "            \"- 图像与相关文本之间的联系。\\n\\n\"\n",
    "            f\"用户提供的关键字: {data_dict['question']}\\n\\n\"\n",
    "            \"文本和/或表格:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "# RAG 管道\n",
    "chain = (\n",
    "    {\n",
    "        \"text_context\": text_retriever | RunnableLambda(split_image_text_types),\n",
    "        \"image_context\": image_retriever | RunnableLambda(split_image_text_types),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(prompt_func)\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1566096d-97c2-4ddc-ba4a-6ef88c525e4e",
   "metadata": {},
   "source": [
    "## 测试检索并运行 RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90121e56-674b-473b-871d-6e4753fd0c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    # 使用 base64 字符串作为源创建 HTML img 标签\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "\n",
    "    # 通过渲染 HTML 显示图像\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "docs = text_retriever.invoke(\"Women with children\", k=5)\n",
    "for doc in docs:\n",
    "    if is_base64(doc.page_content):\n",
    "        plt_img_base64(doc.page_content)\n",
    "    else:\n",
    "        print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eaa532-f035-4c04-b578-02339d42554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = image_retriever.invoke(\"Women with children\", k=5)\n",
    "for doc in docs:\n",
    "    if is_base64(doc.page_content):\n",
    "        plt_img_base64(doc.page_content)\n",
    "    else:\n",
    "        print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb15fd-76fc-49b4-806d-c4db2990027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Women with children\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f08b8-e732-4089-b65c-6eb6f9e48f15",
   "metadata": {},
   "source": [
    "我们可以在 LangSmith 跟踪中看到检索到的图像:\n",
    "\n",
    "LangSmith [跟踪](https://smith.langchain.com/public/69c558a5-49dc-4c60-a49b-3adbb70f74c5/r/e872c2c8-528c-468f-aefd-8b5cd730a673)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
