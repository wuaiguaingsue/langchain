{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ Activeloop çš„ DeepLake è¿›è¡Œé—®ç­”\n",
    "åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Langchain + Activeloop çš„ Deep Lake ä¸ GPT4 ä¸€èµ·å¯¹ç¾¤èŠè¿›è¡Œè¯­ä¹‰æœç´¢å’Œæé—®ã€‚\n",
    "\n",
    "åœ¨[è¿™é‡Œ](https://twitter.com/thisissukh_/status/1647223328363679745)æŸ¥çœ‹æ¼”ç¤ºã€‚"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. å®‰è£…æ‰€éœ€çš„åŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --upgrade langchain 'deeplake[enterprise]' openai tiktoken"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ·»åŠ  API å¯†é’¥"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import DeepLake\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "activeloop_token = getpass.getpass(\"Activeloop Token:\")\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = activeloop_token\n",
    "os.environ[\"ACTIVELOOP_ORG\"] = getpass.getpass(\"Activeloop Org:\")\n",
    "\n",
    "org_id = os.environ[\"ACTIVELOOP_ORG\"]\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "dataset_path = \"hub://\" + org_id + \"/data\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. åˆ›å»ºç¤ºä¾‹æ•°æ®"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æç¤ºé€šè¿‡ ChatGPT ç”Ÿæˆä¸€ä¸ªç¤ºä¾‹ç¾¤èŠå¯¹è¯ï¼š\n",
    "\n",
    "```\n",
    "ç”Ÿæˆä¸€ä¸ªä¸‰ä¸ªæœ‹å‹è°ˆè®ºä»–ä»¬ä¸€å¤©çš„ç¾¤èŠå¯¹è¯ï¼Œå¼•ç”¨çœŸå®åœ°ç‚¹å’Œè™šæ„åå­—ã€‚è®©å®ƒæœ‰è¶£ä¸”å°½å¯èƒ½è¯¦ç»†ã€‚\n",
    "```\n",
    "\n",
    "æˆ‘å·²ç»åœ¨ `messages.txt` ä¸­ç”Ÿæˆäº†è¿™æ ·çš„èŠå¤©è®°å½•ã€‚æˆ‘ä»¬å¯ä»¥ä¿æŒç®€å•ï¼Œåœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ä½¿ç”¨è¿™ä¸ªã€‚\n",
    "\n",
    "## 3. æ‘„å…¥èŠå¤©åµŒå…¥\n",
    "\n",
    "æˆ‘ä»¬åŠ è½½æ–‡æœ¬æ–‡ä»¶ä¸­çš„æ¶ˆæ¯ï¼Œåˆ†å—å¹¶ä¸Šä¼ åˆ° ActiveLoop å‘é‡å­˜å‚¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Participants:\\n\\nJerry: Loves movies and is a bit of a klutz.\\nSamantha: Enthusiastic about food and always trying new restaurants.\\nBarry: A nature lover, but always manages to get lost.\\nJerry: Hey, guys! You won\\'t believe what happened to me at the Times Square AMC theater. I tripped over my own feet and spilled popcorn everywhere! ğŸ¿ğŸ’¥\\n\\nSamantha: LOL, that\\'s so you, Jerry! Was the floor buttery enough for you to ice skate on after that? ğŸ˜‚\\n\\nBarry: Sounds like a regular Tuesday for you, Jerry. Meanwhile, I tried to find that new hiking trail in Central Park. You know, the one that\\'s supposed to be impossible to get lost on? Well, guess what...\\n\\nJerry: You found a hidden treasure?\\n\\nBarry: No, I got lost. AGAIN. ğŸ§­ğŸ™„\\n\\nSamantha: Barry, you\\'d get lost in your own backyard! But speaking of treasures, I found this new sushi place in Little Tokyo. \"Samantha\\'s Sushi Symphony\" it\\'s called. Coincidence? I think not!\\n\\nJerry: Maybe they named it after your ability to eat your body weight in sushi. ğŸ£', metadata={}), Document(page_content='Barry: How do you even FIND all these places, Samantha?\\n\\nSamantha: Simple, I don\\'t rely on Barry\\'s navigation skills. ğŸ˜‰ But seriously, the wasabi there was hotter than Jerry\\'s love for Marvel movies!\\n\\nJerry: Hey, nothing wrong with a little superhero action. By the way, did you guys see the new \"Captain Crunch: Breakfast Avenger\" trailer?\\n\\nSamantha: Captain Crunch? Are you sure you didn\\'t get that from one of your Saturday morning cereal binges?\\n\\nBarry: Yeah, and did he defeat his arch-enemy, General Mills? ğŸ˜†\\n\\nJerry: Ha-ha, very funny. Anyway, that sushi place sounds awesome, Samantha. Next time, let\\'s go together, and maybe Barry can guide us... if we want a city-wide tour first.\\n\\nBarry: As long as we\\'re not hiking, I\\'ll get us there... eventually. ğŸ˜…\\n\\nSamantha: It\\'s a date! But Jerry, you\\'re banned from carrying any food items.\\n\\nJerry: Deal! Just promise me no wasabi challenges. I don\\'t want to end up like the time I tried Sriracha ice cream.', metadata={}), Document(page_content=\"Barry: Wait, what happened with Sriracha ice cream?\\n\\nJerry: Let's just say it was a hot situation. Literally. ğŸ”¥\\n\\nSamantha: ğŸ¤£ I still have the video!\\n\\nJerry: Samantha, if you value our friendship, that video will never see the light of day.\\n\\nSamantha: No promises, Jerry. No promises. ğŸ¤ğŸ˜ˆ\\n\\nBarry: I foresee a fun weekend ahead! ğŸ‰\", metadata={})]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://adilkhan/data', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (3, 1536)  float32   None   \n",
      "    id        text      (3, 1)      str     None   \n",
      " metadata     json      (3, 1)      str     None   \n",
      "   text       text      (3, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "with open(\"messages.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "pages = text_splitter.split_text(state_of_the_union)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "texts = text_splitter.create_documents(pages)\n",
    "\n",
    "print(texts)\n",
    "\n",
    "dataset_path = \"hub://\" + org_id + \"/data\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = DeepLake.from_documents(\n",
    "    texts, embeddings, dataset_path=dataset_path, overwrite=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`å¯é€‰`ï¼šä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ Deep Lake çš„æ‰˜ç®¡å¼ é‡æ•°æ®åº“ä½œä¸ºæ‰˜ç®¡æœåŠ¡å¹¶åœ¨é‚£é‡Œè¿è¡ŒæŸ¥è¯¢ã€‚ä¸ºæ­¤ï¼Œéœ€è¦åœ¨åˆ›å»ºå‘é‡å­˜å‚¨æ—¶æŒ‡å®šè¿è¡Œæ—¶å‚æ•°ä¸º {'tensor_db': True}ã€‚è¿™ä¸ªé…ç½®å…è®¸åœ¨æ‰˜ç®¡å¼ é‡æ•°æ®åº“ä¸Šæ‰§è¡ŒæŸ¥è¯¢ï¼Œè€Œä¸æ˜¯åœ¨å®¢æˆ·ç«¯ã€‚åº”è¯¥æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ªåŠŸèƒ½ä¸é€‚ç”¨äºæœ¬åœ°å­˜å‚¨æˆ–å†…å­˜ä¸­çš„æ•°æ®é›†ã€‚å¦‚æœå‘é‡å­˜å‚¨å·²ç»åœ¨æ‰˜ç®¡å¼ é‡æ•°æ®åº“ä¹‹å¤–åˆ›å»ºï¼Œå¯ä»¥æŒ‰ç…§è§„å®šçš„æ­¥éª¤å°†å…¶è½¬ç§»åˆ°æ‰˜ç®¡å¼ é‡æ•°æ®åº“ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"messages.txt\") as f:\n",
    "#     state_of_the_union = f.read()\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "# pages = text_splitter.split_text(state_of_the_union)\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "# texts = text_splitter.create_documents(pages)\n",
    "\n",
    "# print(texts)\n",
    "\n",
    "# dataset_path = \"hub://\" + org + \"/data\"\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "# db = DeepLake.from_documents(\n",
    "#     texts, embeddings, dataset_path=dataset_path, overwrite=True, runtime={\"tensor_db\": True}\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æé—®\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬å¯ä»¥é€šè¿‡è¯­ä¹‰æœç´¢æå‡ºé—®é¢˜å¹¶å¾—åˆ°ç­”æ¡ˆï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DeepLake(dataset_path=dataset_path, read_only=True, embedding=embeddings)\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs[\"k\"] = 4\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=False\n",
    ")\n",
    "\n",
    "# What was the restaurant the group was talking about called?\n",
    "query = input(\"Enter query:\")\n",
    "\n",
    "# The Hungry Lobster\n",
    "ans = qa({\"query\": query})\n",
    "\n",
    "print(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
