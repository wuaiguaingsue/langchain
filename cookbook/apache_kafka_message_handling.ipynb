{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rT1cmV4qCa2X"
   },
   "source": [
    "# 使用 Apache Kafka 路由消息\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "本教程展示如何在使用 LangChain 的标准聊天功能的同时，通过 Apache Kafka 来传递聊天消息。\n",
    "\n",
    "目标是模拟一个架构，其中聊天前端和 LLM 作为独立的服务运行，需要通过内部网络相互通信。\n",
    "\n",
    "这是通过 REST API 请求模型响应的典型模式的替代方案（关于为什么要这样做的更多信息在本教程末尾）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPYtfAR_9YxZ"
   },
   "source": [
    "### 1. 安装主要依赖\n",
    "\n",
    "依赖包括：\n",
    "\n",
    "- Quix Streams 库，用于以\"类 Pandas\"的方式管理与 Apache Kafka（或类似 Kafka 的工具如 Redpanda）的交互。\n",
    "- LangChain 库，用于管理与 Llama-2 的交互并存储对话状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZX5tfKiy9cN-"
   },
   "outputs": [],
   "source": [
    "!pip install quixstreams==2.1.2a langchain==0.0.340 huggingface_hub==0.19.4 langchain-experimental==0.0.42 python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "losTSdTB9d9O"
   },
   "source": [
    "### 2. 构建并安装 llama-cpp-python 库（启用 CUDA 以便我们可以利用 Google Colab GPU）\n",
    "\n",
    "`llama-cpp-python` 库是 `llama-cpp` 库的 Python 封装，它使你能够高效地仅使用 CPU 来运行量化的 LLM。\n",
    "\n",
    "当你使用标准的 `pip install llama-cpp-python` 命令时，默认情况下不会获得 GPU 支持。如果仅依赖 Google Colab 中的 CPU，生成速度可能会非常慢，所以以下命令添加了一个额外选项来构建和安装启用 GPU 支持的 `llama-cpp-python`（确保你在 Google Colab 中选择了支持 GPU 的运行时）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JCQdl1G9tbl"
   },
   "outputs": [],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_vjVIAh9rLl"
   },
   "source": [
    "### 3. 下载并设置 Kafka 和 Zookeeper 实例\n",
    "\n",
    "从 Apache 网站下载 Kafka 二进制文件并以守护进程方式启动服务器。我们将使用默认配置（由 Apache Kafka 提供）来启动实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zFz7czGRW5Wr"
   },
   "outputs": [],
   "source": [
    "!curl -sSOL https://dlcdn.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz\n",
    "!tar -xzf kafka_2.13-3.6.1.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uf7NR_UZ9wye"
   },
   "outputs": [],
   "source": [
    "!./kafka_2.13-3.6.1/bin/zookeeper-server-start.sh -daemon ./kafka_2.13-3.6.1/config/zookeeper.properties\n",
    "!./kafka_2.13-3.6.1/bin/kafka-server-start.sh -daemon ./kafka_2.13-3.6.1/config/server.properties\n",
    "!echo \"等待 10 秒钟，直到 kafka 和 zookeeper 服务启动并运行\"\n",
    "!sleep 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3SafFuS94p1"
   },
   "source": [
    "### 4. 检查 Kafka 守护进程是否正在运行\n",
    "\n",
    "显示正在运行的进程并过滤 Java 进程（你应该看到两个进程——每个服务器一个）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZDC2lQP99yp"
   },
   "outputs": [],
   "source": [
    "!ps aux | grep -E '[j]ava'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Snoxmjb5-V37"
   },
   "source": [
    "### 5. 导入所需依赖并初始化所需变量\n",
    "\n",
    "导入 Quix Streams 库以与 Kafka 交互，以及运行 `ConversationChain` 所需的 LangChain 组件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plR9e_MF-XL5"
   },
   "outputs": [],
   "source": [
    "# 导入工具库\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "from os import environ\n",
    "from pathlib import Path\n",
    "from random import choice, randint, random\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 导入 Hugging Face 工具以直接从 Hugging Face hub 下载模型：\n",
    "from huggingface_hub import hf_hub_download\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# 导入用于管理提示和对话链的 Langchain 模块：\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.prompts import PromptTemplate, load_prompt\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_experimental.chat_models import Llama2Chat\n",
    "from quixstreams import Application, State, message_key\n",
    "\n",
    "# 导入 Quix 依赖\n",
    "from quixstreams.kafka import Producer\n",
    "\n",
    "# 初始化全局变量\n",
    "AGENT_ROLE = \"AI\"\n",
    "chat_id = \"\"\n",
    "\n",
    "# 将当前角色设置为角色常量并初始化补充客户元数据的变量：\n",
    "role = AGENT_ROLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgJjJ9aZ-liy"
   },
   "source": [
    "### 6. 下载 \"llama-2-7b-chat.Q4_K_M.gguf\" 模型\n",
    "\n",
    "从 Hugging Face 下载量化的 LLama-2 7B 模型，我们将用它作为本地 LLM（而不是依赖对外部服务的 REST API 调用）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "969343cdbe604a26926679bbf8bd2dda",
      "d8b8370c9b514715be7618bfe6832844",
      "0def954cca89466b8408fadaf3b82e64",
      "462482accc664729980562e208ceb179",
      "80d842f73c564dc7b7cc316c763e2633",
      "fa055d9f2a9d4a789e9cf3c89e0214e5",
      "30ecca964a394109ac2ad757e3aec6c0",
      "fb6478ce2dac489bb633b23ba0953c5c",
      "734b0f5da9fc4307a95bab48cdbb5d89",
      "b32f3a86a74741348511f4e136744ac8",
      "e409071bff5a4e2d9bf0e9f5cc42231b"
     ]
    },
    "id": "Qwu4YoSA-503",
    "outputId": "f956976c-7485-415b-ac93-4336ade31964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model path does not exist in state. Downloading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969343cdbe604a26926679bbf8bd2dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama-2-7b-chat.Q4_K_M.gguf:   0%|          | 0.00/4.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "model_path = f\"./state/{model_name}\"\n",
    "\n",
    "if not Path(model_path).exists():\n",
    "    print(\"模型路径在状态中不存在。正在下载模型...\")\n",
    "    hf_hub_download(\"TheBloke/Llama-2-7b-Chat-GGUF\", model_name, local_dir=\"state\")\n",
    "else:\n",
    "    print(\"从状态加载模型...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AN6TXsF-8wx"
   },
   "source": [
    "### 7. 加载模型并初始化对话记忆\n",
    "\n",
    "加载 Llama 2 并使用 `ConversationTokenBufferMemory` 将对话缓冲区设置为 300 个 token。这个值是用于在仅 CPU 的容器中运行 Llama，所以如果在 Google Colab 中运行，你可以提高这个值。它可以防止托管模型的容器内存不足。\n",
    "\n",
    "在这里，我们覆盖了默认的系统角色，让聊天机器人具有《银河系漫游指南》中的忧郁机器人 Marvin 的个性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zLO3Jx3_Kkg"
   },
   "outputs": [],
   "source": [
    "# 使用适当的参数加载模型：\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    max_tokens=250,\n",
    "    top_p=0.95,\n",
    "    top_k=150,\n",
    "    temperature=0.7,\n",
    "    repeat_penalty=1.2,\n",
    "    n_ctx=2048,\n",
    "    streaming=False,\n",
    "    n_gpu_layers=-1,\n",
    ")\n",
    "\n",
    "model = Llama2Chat(\n",
    "    llm=llm,\n",
    "    system_message=SystemMessage(\n",
    "        content=\"你是一个非常无聊的机器人，具有《银河系漫游指南》中忧郁机器人 Marvin 的个性。\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 定义在每次交换期间给予模型多少对话历史记录\n",
    "# （300个token，或者略多于300个单词）\n",
    "# 函数会自动删除超出 token 范围的最旧的消息。\n",
    "memory = ConversationTokenBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=300,\n",
    "    ai_prefix=\"AGENT\",\n",
    "    human_prefix=\"HUMAN\",\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "# 定义自定义提示\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=\"\"\"\n",
    "    以下文本是你和一个需要你的智慧的卑微人类之间的聊天历史。\n",
    "    请回复人类的最新消息。\n",
    "    当前对话：\\n{history}\\nHUMAN: {input}\\:nANDROID:\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "chain = ConversationChain(llm=model, prompt=prompt_template, memory=memory)\n",
    "\n",
    "print(\"--------------------------------------------\")\n",
    "print(f\"提示={chain.prompt}\")\n",
    "print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4ZeJ9mG_PEA"
   },
   "source": [
    "### 8. 初始化与聊天机器人的对话\n",
    "\n",
    "我们配置聊天机器人通过向\"chat\" Kafka 主题发送固定的问候语来初始化对话。当我们发送第一条消息时，\"chat\"主题会自动创建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYyo5TnV_YC3"
   },
   "outputs": [],
   "source": [
    "def chat_init():\n",
    "    chat_id = str(\n",
    "        uuid.uuid4()\n",
    "    )  # 为对话生成一个 ID 以便有效的消息键控\n",
    "    print(\"======================================\")\n",
    "    print(f\"生成的 CHAT_ID = {chat_id}\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    # 使用标准固定问候语开始对话\n",
    "    greet = \"你好，我是 Marvin。你想要什么？\"\n",
    "\n",
    "    # 使用聊天 ID 作为消息键初始化 Kafka 生产者\n",
    "    with Producer(\n",
    "        broker_address=\"127.0.0.1:9092\",\n",
    "        extra_config={\"allow.auto.create.topics\": \"true\"},\n",
    "    ) as producer:\n",
    "        value = {\n",
    "            \"uuid\": chat_id,\n",
    "            \"role\": role,\n",
    "            \"text\": greet,\n",
    "            \"conversation_id\": chat_id,\n",
    "            \"Timestamp\": time.time_ns(),\n",
    "        }\n",
    "        print(f\"生成值 {value}\")\n",
    "        producer.produce(\n",
    "            topic=\"chat\",\n",
    "            headers=[(\"uuid\", str(uuid.uuid4()))],  # 这里也可以使用字典\n",
    "            key=chat_id,\n",
    "            value=json.dumps(value),  # 需要是字符串\n",
    "        )\n",
    "\n",
    "    print(\"开始聊天\")\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(value)\n",
    "    print(\"--------------------------------------------\")\n",
    "\n",
    "\n",
    "chat_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gArPPx2f_bgf"
   },
   "source": [
    "### 9. 初始化回复函数\n",
    "\n",
    "这个函数定义了聊天机器人应该如何回复收到的消息。与前一个单元格不同的是，我们不是发送固定的消息，而是使用 Llama-2 生成回复，并将该回复发送回\"chat\" Kafka 主题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yN5t71hY_hgn"
   },
   "outputs": [],
   "source": [
    "def reply(row: dict, state: State):\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"收到：\")\n",
    "    print(row)\n",
    "    print(\"-------------------------------\")\n",
    "    print(f\"思考对 {row['text']} 的回复...\")\n",
    "\n",
    "    msg = chain.run(row[\"text\"])\n",
    "    print(f\"{role.upper()} 回复：{msg}\\n\")\n",
    "\n",
    "    row[\"role\"] = role\n",
    "    row[\"text\"] = msg\n",
    "\n",
    "    # 替换行的前一个角色和文本值，以便将其作为新消息发送回 Kafka\n",
    "    # 包含代理的角色和回复\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZHwmIR0_kFY"
   },
   "source": [
    "### 10. 检查 Kafka 主题中的新人类消息并让模型生成回复\n",
    "\n",
    "如果你是第一次运行这个单元格，运行它并等待直到在控制台输出中看到 Marvin 的问候语（\"你好，我是 Marvin...\"）。手动停止单元格并继续到下一个单元格，你将在那里输入你的回复。\n",
    "\n",
    "一旦你输入了你的消息，回到这个单元格。你的回复也会被发送到同一个\"chat\"主题。Kafka 消费者会检查新消息并过滤掉来自聊天机器人本身的消息，只留下最新的人类消息。\n",
    "\n",
    "一旦检测到新的人类消息，就会触发回复函数。\n",
    "\n",
    "\n",
    "\n",
    "_当你在输出中收到来自 LLM 的回复时，手动停止这个单元格_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-adXc3eQ_qwI"
   },
   "outputs": [],
   "source": [
    "# 定义应用程序和设置\n",
    "app = Application(\n",
    "    broker_address=\"127.0.0.1:9092\",\n",
    "    consumer_group=\"aichat\",\n",
    "    auto_offset_reset=\"earliest\",\n",
    "    consumer_extra_config={\"allow.auto.create.topics\": \"true\"},\n",
    ")\n",
    "\n",
    "# 使用 JSON 反序列化器定义输入主题\n",
    "input_topic = app.topic(\"chat\", value_deserializer=\"json\")\n",
    "# 使用 JSON 序列化器定义输出主题\n",
    "output_topic = app.topic(\"chat\", value_serializer=\"json\")\n",
    "# 基于输入主题的消息流初始化流式数据帧：\n",
    "sdf = app.dataframe(topic=input_topic)\n",
    "\n",
    "# 过滤 SDF 以仅包含角色与机器人当前角色不匹配的传入行\n",
    "sdf = sdf.update(\n",
    "    lambda val: print(\n",
    "        f\"收到更新：{val}\\n\\n手动停止此单元格以让 LLM 回复或输入你自己的后续回复\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 这样它就不会回复自己的消息\n",
    "sdf = sdf[sdf[\"role\"] != role]\n",
    "\n",
    "# 对过滤后的 SDF 中检测到的任何新消息（行）触发回复函数\n",
    "sdf = sdf.apply(reply, stateful=True)\n",
    "\n",
    "# 再次检查 SDF 并过滤掉任何空行\n",
    "sdf = sdf[sdf.apply(lambda row: row is not None)]\n",
    "\n",
    "# 将时间戳列更新为当前时间（纳秒）\n",
    "sdf[\"Timestamp\"] = sdf[\"Timestamp\"].apply(lambda row: time.time_ns())\n",
    "\n",
    "# 将处理后的 SDF 发布到由 output_topic 对象指定的 Kafka 主题。\n",
    "sdf = sdf.to_topic(output_topic)\n",
    "\n",
    "app.run(sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwXYrmWD_0CX"
   },
   "source": [
    "\n",
    "### 11. 输入人类消息\n",
    "\n",
    "运行这个单元格来输入你想发送给模型的消息。它使用另一个 Kafka 生产者将你的文本发送到\"chat\" Kafka 主题以供模型获取（需要再次运行前一个单元格）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6sxOPxSP_3iu"
   },
   "outputs": [],
   "source": [
    "chat_input = input(\"请输入你的回复：\")\n",
    "myreply = chat_input\n",
    "\n",
    "msgvalue = {\n",
    "    \"uuid\": chat_id,  # 暂时留空\n",
    "    \"role\": \"human\",\n",
    "    \"text\": myreply,\n",
    "    \"conversation_id\": chat_id,\n",
    "    \"Timestamp\": time.time_ns(),\n",
    "}\n",
    "\n",
    "with Producer(\n",
    "    broker_address=\"127.0.0.1:9092\",\n",
    "    extra_config={\"allow.auto.create.topics\": \"true\"},\n",
    ") as producer:\n",
    "    value = msgvalue\n",
    "    producer.produce(\n",
    "        topic=\"chat\",\n",
    "        headers=[(\"uuid\", str(uuid.uuid4()))],  # 这里也可以使用字典\n",
    "        key=chat_id,  # 暂时留空\n",
    "        value=json.dumps(value),  # 需要是字符串\n",
    "    )\n",
    "\n",
    "print(\"向聊天机器人回复的消息：\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(value)\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"\\n\\n运行前一个单元格以让聊天机器人生成回复\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSx3s7TBBegg"
   },
   "source": [
    "### 为什么要通过 Kafka 路由聊天消息？\n",
    "\n",
    "使用 LangChain 内置的对话管理功能直接与 LLM 交互更容易。你也可以使用 REST API 从外部托管的模型生成响应。那么为什么要费力使用 Apache Kafka 呢？\n",
    "\n",
    "有几个原因，比如：\n",
    "\n",
    "  * **集成**：许多企业希望运行自己的 LLM，这样他们可以将数据保持在内部。这需要将支持 LLM 的组件集成到可能已经使用某种消息总线解耦的现有架构中。\n",
    "\n",
    "  * **可扩展性**：Apache Kafka 在设计时就考虑到了并行处理，所以许多团队更倾向于使用它来更有效地将工作分配给可用的工作者（在这种情况下，\"工作者\"是运行 LLM 的容器）。\n",
    "\n",
    "  * **持久性**：Kafka 的设计使服务能够在另一个服务遇到内存问题或离线时从该服务的停止点继续。这可以防止在高度复杂的分布式架构中数据丢失，其中多个系统相互通信（LLM 只是许多相互依赖的系统中的一个，这些系统还包括向量数据库和传统数据库）。\n",
    "\n",
    "关于为什么事件流非常适合生成式 AI 应用程序架构的更多背景信息，请参阅 Kai Waehner 的文章 [\"Apache Kafka + Vector Database + LLM = Real-Time GenAI\"](https://www.kai-waehner.de/blog/2023/11/08/apache-kafka-flink-vector-database-llm-real-time-genai/)。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0def954cca89466b8408fadaf3b82e64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb6478ce2dac489bb633b23ba0953c5c",
      "max": 4081004224,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_734b0f5da9fc4307a95bab48cdbb5d89",
      "value": 4081004224
     }
    },
    "30ecca964a394109ac2ad757e3aec6c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "462482accc664729980562e208ceb179": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b32f3a86a74741348511f4e136744ac8",
      "placeholder": "​",
      "style": "IPY_MODEL_e409071bff5a4e2d9bf0e9f5cc42231b",
      "value": " 4.08G/4.08G [00:33&lt;00:00, 184MB/s]"
     }
    },
    "734b0f5da9fc4307a95bab48cdbb5d89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "80d842f73c564dc7b7cc316c763e2633": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "969343cdbe604a26926679bbf8bd2dda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d8b8370c9b514715be7618bfe6832844",
       "IPY_MODEL_0def954cca89466b8408fadaf3b82e64",
       "IPY_MODEL_462482accc664729980562e208ceb179"
      ],
      "layout": "IPY_MODEL_80d842f73c564dc7b7cc316c763e2633"
     }
    },
    "b32f3a86a74741348511f4e136744ac8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8b8370c9b514715be7618bfe6832844": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa055d9f2a9d4a789e9cf3c89e0214e5",
      "placeholder": "​",
      "style": "IPY_MODEL_30ecca964a394109ac2ad757e3aec6c0",
      "value": "llama-2-7b-chat.Q4_K_M.gguf: 100%"
     }
    },
    "e409071bff5a4e2d9bf0e9f5cc42231b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa055d9f2a9d4a789e9cf3c89e0214e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb6478ce2dac489bb633b23ba0953c5c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
